------------------------------------------------------------------------------------------------------------------------------
                                CI-CD OVERVIEW
------------------------------------------------------------------------------------------------------------------------------

Continuos Integration:
    Push the code to Code Commit/ Github etc.
    Once code pushed, build/test server process the code. 

Continuos Delivery/Deployment:
    Continuos delivery involves manual step. 
    Continuos deployment is fully automated. 

Code Repository:    AWS Code Commit, Git Hub
Build & Test:       AWS Code Build , Jenkins, Bamboo 
Provision:          AWS Elastic Bean Stalk 
Deploy:             AWS Code Deploy (User managed EC2, On Prem, Lambda, ECS)
Orchestrate:        AWS Code Pipeline 

-------------------------------------------------------------------------------------------------------------------------------

Code Commit: 
    It is private fully managed AWS git repository. 
    It is like Git Hub, Bit bucket.
    Code is encrypted and store under the account. 

Code Commit - Create First Repository & HTTPS Configuration:
    It can be connected through HTTPS & SSH Connection.
    SSH is not enabled for root account. 
    Under IAM User -> security credentials -> SSH Keys or HTTPS Git credential for Code Commit. 
    These credentials are used for push or pull from repository. 

    Code Commit -> Create Repository -> (Repository Name) -> Done. 
    Code Commit -> Repository -> View Repository 
    Clone URL gives the repository URL which will copied. 

Code Commit - Clone, Add, Commit, Push:
    git status          -> Shows status whether tracked or untracked. 
    git add .           -> Add files to staging area. 
    git commit -m 'msg' -> Commit the change in local git. 
    git push            -> Push to the repository. During push, asks for username and password. 

Code Commit - Branches and Pull Requests:
    git checkout -b branch_Name     -> It creates new branch and switch to new branch. 
    git branch branch_Name          -> Create a new branch. 
    git checkout branch_Name        -> Switch to new branch. 

    1. Created the new branch and modified with new feature. 
    2. Create Pull request (Merge) -> Merge the new feature branch to master branch. 
    3. In create pull request, select source as 'new feature branch' and destination as 'master' branch. 
    4. If no merge conflicts, will allow to create pull request. 
    5. From pull request section, Reviewer click 'Merge' to accept pull request. (Here have default option to delete the new-feature branch if required)  

Code Commit - Securing the repository and Branches: 
    It can be done by creating the policy to explicit deny to secure master branch. 
    Available in : https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html
    Note: In below policy given for main branch only, Need to add master branch. 

                {
                "Version": "2012-10-17",
                "Statement": [
                    {
                        "Effect": "Deny",
                        "Action": [
                            "codecommit:GitPush",
                            "codecommit:DeleteBranch",
                            "codecommit:PutFile",
                            "codecommit:MergeBranchesByFastForward",
                            "codecommit:MergeBranchesBySquash",
                            "codecommit:MergeBranchesByThreeWay",
                            "codecommit:MergePullRequestByFastForward",
                            "codecommit:MergePullRequestBySquash",
                            "codecommit:MergePullRequestByThreeWay"
                        ],
                        "Resource": "arn:aws:codecommit:us-east-1:*:*",
                        "Condition": {
                            "StringEqualsIfExists": {
                                "codecommit:References": [
                                    "refs/heads/main",
                                    "refs/heads/master"
                                ]
                            },
                            "Null": {
                                "codecommit:References": "false"
                            }
                        }
                    }
                ]
            }


    It failed with below error, so not allowing to push to master branch:

        error: remote unpack failed: internal error
        To https://git-codecommit.us-east-1.amazonaws.com/v1/repos/devops-code-commit-demo
        ! [remote rejected] master -> master (unpacker error)
        error: failed to push some refs to 'https://git-codecommit.us-east-1.amazonaws.com/v1/repos/devops-code-commit-demo'


    Code Commit - Triggers and Notifications:
        Notification - If any process like push, pull etc, it can be notified by using SNS. 
        Trigger - Same as notification but inaddition to SNS it allows to trigger Lambda. 
        Cloud watch Event - It can also done here by select source as code commit and target as any like SNS, Lambda etc. 

        Code Commit -> Settings -> Notifications Tab:
            Name, Events, SNS Target to select. 

        Code Commit -> Settings -> Trigger Tab: 
            Can select SNS or Lambda. 
        
        Event Bridge -> Rules -> Create Rule: 
            Name, Service Provider, Target 
    
    Code Commit - AWS Lambda: 
        https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html
        
        1. Created the lambda function with trigger as code commit. 
        2. If any change in events in code commit, triggered the lambda. 
        3. Verified in cloud watch logs. 


-------------------------------------------------------------------------------------------------------------------------------

Code Build:
    It is fully managed build service like Jenkins. 
    Leverages Docker under the hood. 
    Source code from S3, Code Commit, Code Pipeline etc. 
    Build instruction defined in buildspec.yml file. 
    Output to S3 and cloud watch logs. 
    Can integrate with Event bridge to detect failures. 
    SNS & Lambda integrations. 

Code Build - First Build: 
    code build -> build projects -> 
    Sources - No sources/S3/Code Commit/Git hub/Bit bucket. 
    Reference Type can be Branch/Git tag/ Commit ID 
    Once build created, click 'start build' to run the build.                                                   


Code Build - buildspec.yml file: 
    It can be placed in S3 or root of source directory.

        version: 0.2                    # represent buildspec version. recommended to use 0.2 
        run-as: Linux-user-name         # optional specific to linux. 
        env:
        shell: shell-tag
        variables:
            key: "value"
            key: "value"
        parameter-store:
            key: "value"
            key: "value"
        exported-variables:
            - variable
            - variable
        secrets-manager:
            key: secret-id:json-key:version-stage:version-id
        git-credential-helper: no | yes

        proxy:
        upload-artifacts: no | yes
        logs: no | yes

        batch:
        fast-fail: false | true
        # build-list:
        # build-matrix:
        # build-graph:
                
        phases:
        install:
            run-as: Linux-user-name
            on-failure: ABORT | CONTINUE
            runtime-versions:
            runtime: version
            runtime: version
            commands:
            - command
            - command
            finally:                # this will be executed if previous is failed or success. 
            - command
            - command
        pre_build:
            run-as: Linux-user-name
            on-failure: ABORT | CONTINUE
            commands:
            - command
            - command
            finally:
            - command
            - command
        build:
            run-as: Linux-user-name
            on-failure: ABORT | CONTINUE
            commands:
            - command
            - command
            finally:
            - command
            - command
        post_build:
            run-as: Linux-user-name
            on-failure: ABORT | CONTINUE
            commands:
            - command
            - command
            finally:
            - command
            - command
        reports:
        report-group-name-or-arn:
            files:
            - location
            - location
            base-directory: location
            discard-paths: no | yes
            file-format: report-format
        artifacts:              
        files:
            - location
            - location
        name: artifact-name
        discard-paths: no | yes
        base-directory: location
        exclude-paths: excluded paths
        enable-symlinks: no | yes
        s3-prefix: prefix
        secondary-artifacts:
            artifactIdentifier:
            files:
                - location
                - location
            name: secondary-artifact-name
            discard-paths: no | yes
            base-directory: location
            artifactIdentifier:
            files:
                - location
                - location
            discard-paths: no | yes
            base-directory: location
        cache:
        paths:
            - path
            - path


Code Build - Docker, ECR & buildspec.yml file: 
    Code build used to build docker image and push to ECR. 
    
    1. Created the Dockerfile and buildspec.yml file. 
    2. Docker File: 
        FROM node:12-alpine
        RUN apk add --no-cache python3 g++ make
        WORKDIR /app
        COPY . .
        RUN yarn install --production
        CMD ["node", "src/index.js"]
    3. buildspec.yml: 
        version: 0.2
        phases:
        pre_build:
            commands:
            - echo Logging in to Amazon ECR...
            - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com
        build:
            commands:
            - echo Build started on `date`
            - echo Building the Docker image...          
            - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .
            - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG      
        post_build:
            commands:
            - echo Build completed on `date`
            - echo Pushing the Docker image...
            - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG

    4. zip myzip.zip Dockerfile buildspec.yml 
    5. Use the environemental variables IMAGE_REPO_NAME=ECS Repository Name , IMAGE_TAG=latest version. 
    6. After build completed, ECR -> Repositories -> Images. 


Code Build - Environmental Variable and Parameter Store:
    https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html
    Link has list of environmental variables can use. 
    printenv in buildspec.yml (under commands) file prints all environmental variables. 
    environment variables can be given in buildspec file or override in console. 
    Parameters can be place from Systems Manager Parameter Store or Secrets Manager. 

Code Build - Artifacts & S3: 
    1. Include the artifacts in buildspec.yaml file. 
    2. In code build console, update to include the artifacts by giving s3.  
    3. In code build, start the build. 
    4. Artifacts created in S3. 

Code Build - Cloud watch Events, Cloudwatch Logs, Cloud watch metrics & Triggers:
    Code build -> View Projects -> edit -> Logs: 
        can select the cloudwatch logs and also s3 bucket to store logs. 
    
    Cloud watch -> Metrics -> Code build -> Select by Project or Account level. 
    Metrics can export to dashboard. 

    Cloud watch -> Events -> Rules: (Now changed to Amazon event bridge): 
        Event bridge -> Name, Event Pattern or Schedule (use cron to run at specific time) -> create Rule. 
    
Code Build - Validating code commit Pull Requests: 
    https://aws.amazon.com/blogs/devops/validating-aws-codecommit-pull-requests-with-aws-codebuild-and-aws-lambda/

-------------------------------------------------------------------------------------------------------------------------------

Code Deploy:
    EC2 or On-prem machine must running code deploy agent. 
    Agent continuosly polls code deploy for work. 
    code deploy has appspec.yml file. 
    Application is pulled from s3 or git hub. 
    Ec2 instances are group by deployment group. 
    It can integrate with pipeline. 
    Blue-Green works only with EC2 (not on-prem) 
    It support for Lambda, EC2. 

    Code + appspec.yml to S3 <->(Pulled by agent) EC2+agent. So EC2 requires access to read from S3. 

Code Deploy - EC2 Setup: 
    1. Create EC2 instance with TAGS mandatory. Tags are used by deployment group in code deploy service. 
    2. Create a service role with S3 access which requires to pull code_appspec.yml file from S3. 
    3. Install the code deploy agent in EC2. Either after ssh or use in user data field. 
            sudo yum update -y
            sudo yum install -y ruby wget
            wget https://aws-codedeploy-eu-west-1.s3.eu-west-1.amazonaws.com/latest/install
            chmod +x ./install
            sudo ./install auto
            sudo service codedeploy-agent status

Code Deploy - Application, Deployment Groups & First Deployment: 
    1. Created the role (Allow code deploy to access EC2).
    2. Created the application. 
    3. Create the deployment group: 
            Name, Deployment type (Inplace or Blue green), Deployment Settings (Allatonce, oneatatime, halfatatime)
    4. Upload the code + appspecfile in zip to S3. 
            aws deploy push --application-name CodeDeployDemo --s3-location s3://aws-devops-course-stephane/codedeploy-demo/app.zip --ignore-hidden-files --region eu-west-1 --profile aws-devops            
    5. Create deployment: 
            s3 bucket location which contain code and then create deployment. 

    Note: To check whether code deploy agent is properly installed. If not installed, then deploy will timeout with error. 

Code Deploy - Deployment Groups: 
    Under application, can create multiple deployment group with different tags. 
    This helps to separate for dev, prod instances. 
    During deploy, can select the specific deployment group for deployment. 

Code Deploy - Deployment Group Configurations: 
    Deployment Type: 
        Inplace - In existing instances, update the applications. 
        Blue Green  - Create new instances through manually or AutoScaling options. 
    
    Deployment Configurations: 
        AllatOnce, OneataTime, HalfataTime, Manually create deployment configurations. 
    
    Loadbalancer: 
        For blue-green, it is mandatory. 
        For inplace, it is optional. 

Code Deploy - appspec.yml file: 

    https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html

    Note: Since run as root, no need of sudo in scripts. 

    version: 0.0
    os: linux
    files:
    - source: /
        destination: /var/www/html/WordPress
    hooks:
    BeforeInstall:
        - location: scripts/install_dependencies.sh
        timeout: 300
        runas: root
    AfterInstall:
        - location: scripts/change_permissions.sh
        timeout: 300
        runas: root
    ApplicationStart:
        - location: scripts/start_server.sh
        - location: scripts/create_test_db.shhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html

    Hooks availabel different for , Inplace, Blue green original & replacement, Lambda  & ECS. 
    Few environmental variables are available like APPLICATION_NAME, DEPLOYMENT_ID
    
    if [ "$DEPLOYMENT_GROUP_NAME" == "Staging" ]
    then
        sed -i -e 's/Listen 80/Listen 9090/g' /etc/httpd/conf/httpd.conf
    fi
        timeout: 300
        runas: root
    ApplicationStop:
        - location: scripts/stop_server.sh
        timeout: 300
        runas: root
        
Code Deploy - Hooks & Environmental Variables: 
    https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html

    Hooks availabel different for , Inplace, Blue green original & replacement, Lambda  & ECS. 
    Few environmental variables are available like APPLICATION_NAME, DEPLOYMENT_ID, DEPLOYMENT_GROUP_NAME, DEPLOYMENT_GROUP_ID, LIFECYCLE_EVENT 
    
    if [ "$DEPLOYMENT_GROUP_NAME" == "Staging" ]
    then
        sed -i -e 's/Listen 80/Listen 9090/g' /etc/httpd/conf/httpd.conf
    fi

Code Deploy - Cloud watch, Alarm, Trigger:
    Code Deploy -> Deployment groups -> Advanced Option -> Trigger 
    Code Deploy -> Deployment groups -> Advanced Option -> Alarm 
    Event bridge -> Code build -> Target can be any. 

Code Deploy - Rollbacks: 
    Code Deploy -> Deployment groups -> Advanced Option -> Rollbacks. 
    1. Rollback when deployment fails. 
    2. Rollback when alarm threshold are met. (This can use to monitor cpu utiliation, if more than to rollback)

Code Deploy - On Premise Setup: 
    https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-on-premises.html

    Step 1 – Configure each on-premises instance, register it with CodeDeploy, and then TAG it.
    Step 2 – Deploy application revisions to the on-premises instance.

    1. use IAM User ARN  for authenticate request. -> Best for smaller instances. 
    2. use IAM role ARN for authenticate request. -> This is secure and complex. 

Code Deploy - Deploy to Lambda: 
    https://docs.aws.amazon.com/codedeploy/latest/userguide/applications-create-lambda.html

    hooks: 
        beforeallowtraffic functionname
        afterallowtraffic functionname 
    
    1. Create a service role for code-deploy for lambda. 
    2. Create the application. 
    3. Creaet the deployment group. 
    4. Deployment type: 
            allataonce
            canary - shifts in two increment 
            linear - shifts in equal increment. 
    5. Hooks contains beforeallowtraffic & afterallowtraffic. 

-------------------------------------------------------------------------------------------------------------------------------

Code Pipeline: 
    It is CI-CD Orchestrate tool. 
    Source: S3, Code Commit, ECR, Git hub, Bitbucket. 
    Build:  Code Build, Jenkins
    Test:   3rd party tools, 
    Deploy: Code deploy, Cloud formation, Elastic bean stalk, ECS, S3 etc.

    It is made up of stages and each stages have sequential or parallel actions. 
    Can have manual approval process. 
    For each stage, generates output artifacts which will pass as input artifacts to next stage. 
    Atlest 2 stage is mandatory. 

Code Pipeline - Code Commit & Code Deploy: 
    In source, can select code commit, give the repository and branch details. 
    Note: Pipeline can created for 1 branch only. 
    Change detection Options: 
        Amazon cloudwatch events (recommended and automatically detect changes)
        AWS Code pipeline  (It periodically checks for changes)

    In code pipeline, can deploy to another region. 

Code Pipeline - Adding Code Build: 
    Can edit code pipeline. 
    Stage can be like source, build, test etc 
    Action group can be parallel or sequential. 

Code Pipeline - Artifacts, Encryption & S3: 
    Code pipeline -> Advanced setting: 
    S3: 
        Default location or Custom Location (recommended)    
    Key: 
        Default AWS Managed key or Customer managed key 

Code Pipeline - Manual approval steps:
    Under pipeline -> edit stage -> Manual approval stage. 
    Can select SNS topic to send notification. 
    
Code Pipeline - Cloud watch events integration: 
    During pipeline creation, it automatically create rules in event bridge. 
    Also can create our own rule, to choose target. 

Code Pipeline - Stage actions , Sequential & Parallel: 
    It consists of stages like source, build etc. 
    Sequential actions -> It process sequentially 
    Parallel actions -> It process parallel. 
    runOrder 1 
    runOrder 2 (It can be multiple and run parallel)
    runOrder 3 (It run after 2 and multiple can run parallel)

Code Pipeline - All Integrations: 
    https://docs.aws.amazon.com/codepipeline/latest/userguide/best-practices.html

    Build - Code build -> Generate artifacts 
    Test - Code build -> Does NOT generate artifacts. 

    Deploy -> Cloud formation (Comman use cases) 

Code Pipeline - Custom Action jobs with Lambda: 
    https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html

    1. Created the lambda service role. 
    2. Created the lambda function. 
    3. In code pipeline, Invoke a lambda function. 

Code Pipeline - Cloud Formation: 
    https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-cloudformation.html

-------------------------------------------------------------------------------------------------------------------------------

Code Star - Overview: 
    Just upload a project and Code star will create the pipeline automatically which includes code commit, code build, code deploy etc depends upon the project. 

-------------------------------------------------------------------------------------------------------------------------------

Jenkins Architecture: 
    Jenkins is open source CI-CD tool.
    Can replace code build, deploy, code pipeline. 
    Must deploy in master slave configuration. 
    Have jenkinsfile similar to buildspec.yml file. 
    Jenkins can replace code pipeline or other like code build, code deploy etc. 

Jenkins - Setup on EC2: 
    #!/bin/bash
    # setup Jenkins on EC2
    sudo yum update -y
    sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo
    sudo rpm --import https://pkg.jenkins.io/redhat/jenkins.io.key
    sudo yum install java-1.8.0 -y
    sudo amazon-linux-extras install epel
    sudo yum install jenkins -y
    sudo service jenkins start

    sudo cat /var/lib/jenkins/secrets/initialAdminPassword

    ec2-public-ip-address:8080 port 
    
Jenkis - AWS Plugins: 
    In Jenkins, have different plugins to integrate with AWS like AWS EC2, Code Pipeline etc. 

White Papers to Read: 
    Whitepapers to Read
        At this stage, please have a look at the following whitepapers to better help you in preparing for your certification. You don't need to read everything, just understand the general idea and skim through.

        MUST READ - Blue/Green Deployments on AWS
        https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf

        RECOMMENDED - Practicing Continuous Integration Continuous Delivery on AWS
        https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf

        RECOMMENDED - Jenkins on AWS
        https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf

        OPTIONAL - Introduction to DevOps on AWS
        https://d1.awsstatic.com/whitepapers/AWS_DevOps.pdf

        OPTIONAL - Development and Test on AWS
        https://d1.awsstatic.com/whitepapers/aws-development-test-environments.pdf


-------------------------------------------------------------------------------------------------------------------------------
                    CONFIGURATION MANAGEMENT & INFRASTRUCTURE AS CODE
-------------------------------------------------------------------------------------------------------------------------------

Cloud Formation: 
    Infrastructure as code 
    version control 
    Cant modify the existing stack. Can reupload again. 

Cloud Formation - Status: 
    CREATE_IN_PROGRESS 
    CREATE_COMPLETE 
    CREATE_FAILED
    UPDATE_IN_PROGRESS
    UPDATE_COMPLETE
    UPDATE_COMPLETE_CLEANUP_IN_PROGRESS 
    UPDATE_ROLLBACK_COMPLETE
    UPDATE_ROLLBACK_FAILED  -> it is important 
    DELETE_IN_PROGRESS
    DELETE_COMPLETE
    DELETE_FAILED
    ROLLBACK_IN_PROGRESS 
    ROLLBACK_COMPLETE 
    ROLLBACK_FAILED 

Cloud Formation - Create stacks hands on: 
    Cloudformation -> create stack -> S3 URL or Upload template. 
    Tags - Apply to the underlying resources. 
    Permissions - policy 
    Stack Failure - Roll back or Preserve. 
    Stack Policy - Defines the resources that want to protect from unintentional updates. 
    Timeout 
    Notifications 
    Termination protection 

    Estimate cost option. 

Cloud Formation - Update and Delete Stack: 
    Modify allow to upload new template or replace existing template. 
    Delting the stack, delete the underlying resources. 

Cloud Formation Parameters: 
    Parameters help to input custom values to stack. Some inputs cannot be determine at ahead of time. 

    Pseudo parameter are used to get the default values:  It refer by using !Ref AWS:AccountId 
        AWS::AccountId 
        AWS::NotificationARNs 
        AWS::NoValue 
        AWS::Region 
        AWS::StackId 
        AWS::StackName 

    Parameters:
        InstanceTypeParameter:
            Type: String
            Default: t2.micro
            AllowedValues:
            - t2.micro
            - m1.small
            - m1.large
            Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro.
            
Cloud Formation Resources: 
    It helps to create the resources. 
    AWS::aws-product-name::data-type-name 

    Resources: 
        myEC2:
            Type: AWS::EC2::Instance
            Properties: 
            ImageId: 'ami-0ed9277fb7eb570c9'
            InstanceType: !Ref InstanceTypeParameter

    Can create dynamic resource: 
        No, cannot generate. Everything has to be declared. 
    
    Is every AWS service supported: 
        Almost. For unavailable can use Lambda custom resources. 

Cloud Formation Mappings: 
    Used for known value in advance. 
    It is key value pair based. 
    !FindInMap[map_name, top_level_key, 2nd_level_key]      -> To retrieve the values from map. 

    Mappings: 
        Mapping01: 
            Key01: 
              Name: Value01
            Key02: 
              Name: Value02
            Key03: 
              Name: Value03
            
    RegionMap: 
        us-east-1:
            HVM64: ami-0ff8a91507f77f867
            HVMG2: ami-0a584ac55a7631c0c
        us-west-1:
            HVM64: ami-0bdb828fd58c52235
            HVMG2: ami-066ee5fd4a9ef77f1
        eu-west-1:
            HVM64: ami-047bb4163c506cd98
            HVMG2: ami-0a7c483d527806435
        ap-northeast-1:
            HVM64: ami-06cd52961ce9f0d85
            HVMG2: ami-053cdd503598e4a9d
        ap-southeast-1:
            HVM64: ami-08569b978cc4dfa10
            HVMG2: ami-0be9df32ae9f92309
                
Cloud Formation - Outputs: 
    If output is export (export name), then can import in another stack. (!importvalue export-name) 
    It enables cross stack. 
    Cant delete a stack if output is references in another stack. 

    Outputs: 
        Outputdisplayavailabilityzone: 
            Description: Output value description
            Value:  !GetAtt myEC2.AvailabilityZone
            Export: 
            Name: Availability-zone
        Outputdisplayinstanceid: 
            Description: Output value description
            Value:  !Ref myEC2
            Export: 
            Name: Instance-ID 

Cloud Formation - Conditions: 
    Can define the condition, if condition is true then only resource or output can be created. 
    Condition can reference another condition, parameter value or mapping. 

Cloud Formation Intrinsic Functions: 
        Fn::And
        Fn::Equals
        Fn::If
        Fn::Not
        Fn::Or
    Fn::Base64              -> returns base64 representation of input string. used to pass user data. 
    Fn::Ref or !Ref         -> In resources, reference the resource ID. In parameters, references the value. 
    Fn:GetAtt or !GetAtt    -> To get the attributes of the resources. 
    Fn::FindInMap or !FindInMap -> !FindInMap [map-name, top-key, 2nd-key]
    Fn::Join                -> !Join [delimeter, [list of values ]]
    Fn::Sub                 ->  substitute 

Cloud Formation - User data: 
    It must be under Fn::Base64 
    User data is stored under /var/log/cloud-init log or /var/log/cloud-init-output.log 

          UserData:
            Fn::Base64: !Sub |
                #!/bin/bash -xe
                yum update -y 
                yum install cloud-init -y
                yum install httpd -y

Cloud Formation - cfn-init: 
    AWS::CloudFormation::Init must be in METADATA of a RESOURCE.
    It helps to make complex configuration readable. 
    EC2 instance query the cloudformation template to get the init data. 
    Logs captured in /var/log/cfn-init logs. 

Cloud Formation - cfn signal and wait conditions: 
    After cfn-init, still dont know whether ec2 metadata are properly installed or not. 
    for this, cfn-signal is used to tell cloudformation stack about the success. 
    Need to define WaitCondition, it blocks template from process and wait for signal. 
    cfn-signal to process followed by cfn-init. 

Cloud Formation - wait signal trouble shoot: 
    cfn init helper script not installed. 
    check the /var/log/cloud-init.log or cfn-init.log 
    disable rollback on failure to check the logs. 
    verify has internet connection. 

Cloud Formation - rollbacks: 
    During stack creation -> Stack failure options: 
        1. Rollback all stack resources. 
        2. Preserve successfully provisioned resources. 

Cloud Formation - Nested Stacks: 
    Nested stacks are stacks are created separately and referenced in stack. 
    As code grows and separate stacks for network, server, database etc. reference the stack with in another stack. 

    AWS::CloudFormation::Stack 
    CAPABILITY_AUTO_EXPAND 
    IAM_CAPABILITY 

Cloud formation - Change Sets: 
    It helps to verify the change before implement the stack. 
    Cloudformation -> During create process -> Create change set .
    After, Under change sets -> Execute to run the stack. 

Cloud Formation - Deletion Policy: 
    Retain 
    Snapshot 
    Delete 
    DeletionPolicy: Retain

Cloud Formation - Termination Protection: 
    Cloud Formation -> Creation -> Stack Creation Options -> Termination Protection (Enabled or Disabled)


Cloud Formation - Parameters from SSM: 

    Type:   AWS::SSM::Parameter                   -> To create the parameter. 
    Type:   AWS::SSM::Parameter::Value<String>    -> To get the value         
            AWS::SSM::Parameter::Value<List<String>>
            AWS::SSM::Parameter::Value<Any AWS type>
            'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'


Cloud Formation - Public Parameters from SSM: 
    aws ssm get-parameters-by-path --path /aws/service/ami-amazon-linux-latest --query 'Parameters[].Name' 
    This command helps to get the list of parameters.  Use this in the Default value. 

Cloud Formation - DependsOn: 
    Resources are created parallely. 
    DependsOn helps to create dependency between resources. 

Cloud Formation - Deploying Lambda Function: 
    Role ARN is required. 
    handler - index.handler_name 
    In lambda, 4000 characters have inline. Only node.js and python allowed as inline. 
    CAPABILITY_IAM is required when create the IAM role. 

    To get the source from S3 bucket: 
        place the code in index.py file and zipped to s3 bucket. 
              Code: 
                S3Bucket: devops-code-build-image-code-source
                S3Key: lambda-code.zip
                S3ObjectVersion: pXPqiF6GWRdBg3TKjZlHoNhCTKcB7qHh

Cloud Formation - Custom Resources: 
    Custom resources used in below scenarios: 
        1. Provisioning AWS resources that are not supported in cloud formation. 
        2. Provisioning non AWS Resources. 
        3. Perform provisioning steps not related to infrasture. 
                ex: initialize the database. 
    
    It consists of: 
        1. Create the logic for custom resources. 
        2. Deploy the custom resource to Lambda function. 
        3. Use the custom resource in cloud formation template that references lambda function or SNS topic. 
        
        To use a custom resource in a CloudFormation stack, you need to create a resource of either type AWS::CloudFormation::CustomResource or Custom::<YourName>. 
        ServiceToken: ARN of Lambda function. 

        Cloudformation runs logic at anytime during create, update and delete stacks. 


Cloud Formation - Drift Detection: 
    Cloudformation -> Stack actions -> Detect drift 
    Cloudformation -> Stack actions -> View drift results 

    If any change in cloudformation template and actual infrastructure. then capture in drift detection. 

Cloud Formation - Insufficient Capabilities Exception: 
    CAPABILITY_IAM or CAPABILITY_NAMED_IAM (for custom resources) -> To create IAM resources.  (InsufficientCApabilitiesException)
    CAPABILITY_AUTO_EXPAND -> For nested stack. 

Cloud Formation - cfn hup and metadata: 
    If any change in metadata, cfn-hub will detect the metadata changes with default interval of 15 mins and run the cloudformation::init metadata again. 
    /etc/cfn/cfn-hup.conf 
    /etc/cfn/hooks.d/cfn-auto-reloader.conf 

Cloud Formation - Stack Policies: 
    It provides fail safe mechanism to prevent accidental deletion. 
    It helps to prevent update or delete through stack policy. 
    It allow to update but during process failed with error "Action denied by stack policy: Statement [#1] does not allow [Update:*] for resource [*];"

-------------------------------------------------------------------------------------------------------------------------------

Elastic Bean Stalk: 
    It is easy to use for deploying and scaling web applications. 
    Simply upload the code and EBS automatically handles the deployment, load balancing, auto scaling to health monitoring. 
    No charge for EBS but only for underlying resources. 

Console Process:     
    Applications: 
        1. Create Application: 
            Name, Platform, Application (sample or upload own code)
    Environments: 
        Web server environment 
        Worker environment 
      
    Change History: 

Elastic Beanstalk - CLI: 
    https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install-advanced.html   -> To run manually eb 

    https://github.com/aws/aws-elastic-beanstalk-cli-setup -> Execute scripts to install eb 

    Initialize EB CLI Project: 
        eb init         -> Initialize new project (give application name, platform)
        eb create       -> To create the environment. 
        eb status       -> To see the current status of environment. 
        eb health       -> To check the health information of instance in environment. 
        eb events       -> List of events output in EB. 
        eb logs         -> To pull the logs from instance in environment. 
        eb open         -> To open environment website in browser. 
        eb deploy       -> To deploy the application in environment. 
        eb config       -> to check the configuration details. 
        eb terminate    -> terminate the environment 
    
Elastic Beanstalk - Saved Configurations: 
    Save configuration helps to save the environment configuration details and later use the configuration to CREATE the new environment or LOAD to existing environment. 

    https://aws.amazon.com/blogs/devops/using-the-elastic-beanstalk-eb-cli-to-create-manage-and-share-environment-configuration/

    Environment -> Actions -> Save Configuration. 

    eb config save environment-name --cfg config-name   -> Save the current config of environment. 
    eb setenv ENABLE_COOL_NEW_FEATURE=true              -> Set the new config in current environment. 
    eb config --cfg config-name                         -> Update the config to the envionment. 

Elastic Beanstalk - .ebextensions for configs: 
    Under .ebextensions -> name.config file. 
    option_settings key used to modify EB configuration and define variables that can be retrieved from application using environemental variables. 

    option_settings:
        - namespace:  namespace
            option_name:  option name
            value:  option value
        - namespace:  namespace
            option_name:  option name
            value:  option value
            
    namespace is optional. 

    Precedence: 
        https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html

        Default Value -> Configuration file (.ebextensions/name.confg) -> saved configuration -> Setting applied during creation/update thorugh console, CLI, SDK. 

Elastic Beanstalk - .ebextensions for Resources: 
    Under .ebextensions -> name.config file. 
    Can define the cloudformation template to create resources as part of elastic beanstalk. 

Elastic Beanstalk - RDS in or out of environment: 
    If RDS is part of EB, then will be deleted. 
    To Overcome: 
        1. Create external table. 
        2. Under EB Configuration -> Use key and value with dynamodb table name. 

Elastic Beanstalk - .ebextensions for commands and containers: 
    commands: 
        run on instance before application is setup. 
    container_commands: 
        It helps to modify source code. 
        It runs after application code is extracted and before the application version deploy. 
        It run after application is setup. 
        database_migration: 
            leader_only: true -> to run only on single instance with leader only. 
            
Elastic Beanstalk - Good Features to know: 
    Clone Environment - To duplicate the environment. 
    Rebuild Environment - To delete and recreate the environment. 
    Application -> Settings -> Application version lifecycle settings -> TO limit the no of versions for future deployments. 

Elastic Beanstalk- Rolling update strategy: 
    Environment -> Configuration -> Rolling updates and deployment. 
        Deployment Policy: 
            All at once -> Deploy all in one go. Fastest deployment. Downtime. No additional cost. 
            Rolling  -> update few instance at a time. Run below capacity. 
            Rolling with additional batch  -> Few additional instances. Run at capacity, small additional cost. Longer deploy. 
            Immutable   -> Temporary ASG is created. Higher cost. Longest deploy. Quick rollback. 
            Traffic Splitting 

            SWAP URL is used to shift traffic between environment. 

    https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html?icmpid=docs_elasticbeanstalk_console

    Deployment Modes: 
        Single Instance   -> Have single instance and good for dev. 
        High Availability -> Have load balancer and good for prod. 
    
Elastic Beanstalk - Rolling updates hands on: 
    Environment -> COnfiguration -> Rolling updates and deployment. :
        Application Deployments & Configuration updates. 

Elastic Beanstalk - SWAP URL: 
    Requires 2 environment and swap the URL between the environment. 
    1. Used clone option to created the similar environment. 
    2. Updated the source code. 
    3. eb deploy environment-name (to deploy the application to new environment)

    Environment -> Actions -> SWAP URL -> Select environment to swap. 

Elastic Beanstalk - Worker Environment: 
    For long running workloads or schedule using cron.yml 
    It polls from SQS. 

Elastic Beanstalk - Multi Docker Integration: 
    To create environment for docker: 
        Docker  -> Single container 
        Multi Container Docker: 
            Dockerrun.aws.json file -> Configuration file . 

-------------------------------------------------------------------------------------------------------------------------------

Lambda: 
    This is serverless. 
    Default create outside of VPC but can tag to VPC. 
    It has monitoring. 

Lambda - Sources and Use cases: 
    Source trigger can be API gateway, Cloud watch events, Alexa, Load balancer , Code commit, dynamodb, s3, kinesis etc. 

Lambda - Security, Environmental variables, KMS and SSM: 
    It support secrets managers, parameter store, environmental variables 

Lambda - Versions, Aliases and Canary Routing: 
    When work on lambda, $LATEST -> Mutable. 
    Version (code + configuration)  - Immutable. 
    $LATEST (mutable) -> Version V1 (immutable)
    Alias are pointers which point to versions and mutable. 
    Alias enabled blue green deployment by shifting the traffic to versions. 

Lambda - SAM Framework: 
    Serverless Application Model (SAM) is CLI tool helps to create and manage serverless service. 
    sam init        -> Initialize new project. 
    sam build       -> Build the project 
    sam package     -> Package the project. 
    sam deploy      -> Deploy the application. 

    SAM comes with build in Code Deploy to deploy the appliations. 

-------------------------------------------------------------------------------------------------------------------------------

Step Functions: 
    It helps to create workflow. 
    State machine defined using amazon state language. 
    It runs upto one year. 

-------------------------------------------------------------------------------------------------------------------------------

API Gateway: 
    It supports HTTP, REst API, WebSockets. 
    Act as front door. 
    It integrate with Lambda, Mock, Other AWS Services. 

    Endpoint Type : Regional, Edge Optimized Endpoint (edge location), Private (with vpc)

    Actions -> Create Methods (GET, PUT etc )

    Integration Type: 
        Lambda Function 
        HTTP 
        Mock 
        AWS Service 
        VPC Link 

        Method Request/Method Response , Integration Request/Integration Response

        Deploy API -> Deploy to Stages 
    
    API Keys -> Create API Keys. 

    Authenticator - Using Cognito user Pools or Lambda authorizer. 

API Gateway - Integration with Lambda: 
    If proxy is enabled: 
        Lambda_Proxy -> Entire request passed to Lambda. 
            Modification is not allowed in Integration request. 
        
    Function_name = function_name:alias/version   (Can give alias or version in function name)
    Mapping template - It helps to modify the content of the request sent to function or return from function. 

API Gateway - Stages & Deployment: 
    Requires deployment for changes to effect. 
    Can create multiple stages. Each having their own stage variable (environment variable). 

API Gateway - Canary Deployments: 
    Can enable canary deployment for any stage. 
    Choose % of traffic that canary receives. 
    Promote Canary is used to deploy canary. 

    API Gateway -> Stages -> Canary -> Create canary. 
    Once canary created, then showing the stages with Canary options. 

API Gateway - Throttles: 
    10,000 request per second. 
    Usage Plan (Gives the request) -> Associate key with usage. 

API Gateway - Fronting Step Functions: 
    API Gateway -> AWS Services (Step Functions)

-------------------------------------------------------------------------------------------------------------------------------

ECS (Elastic Container Service):

Docker: 
    It is used to deploy apps. 
    Package apps in containers which run on any OS under the hood used Linux. 
    Docker images are stored in Docker Repositories like Docker hub (public), ECR (private).
    Docker used docker daemon engine to run the images (container) on same server. 
    Dockerfile used to create docker image. 

    docker build    -> Build image fromDockerfile 
    docker run      -> Run the container 
    docker push     -> Push to docker hub 
    docker pull     -> Pull from docker image

    It consists of: 
        ECS 
        Fargate 
        EKS 

ECS Clusters: 
    It is logical grouping of EC2 instances.
    EC2 instances run the ECS agent (Docker Container) 
    ECS agent register the instance to clusters. 

    ECS -> Clusters -> Create Cluster: 
        Cluster Template: 
            Networking only (Fargate) 
            EC2 Linux + Networking 
            EC2 Windows + Networking 

    Under etc/ecs/ecs.config file: ECS_CLUSTER is used to register the instance to cluster. 
    cat /etc/ecs/ecs.config 
    ECS_CLUSTER=ecs-cluster-demo
    ECS_BACKEND_HOST=
    ECS Instances under the cluster. 

    It create the instance as Auto scaling group. If require to modify the instances, can change in auto scaling group. 


ECS Task Definition: 
    It contains metadata in json format which tells ECS how to run container. 
    It contains crucial information around: 
        Image name 
        Port binding for container and hosts. 
        Memory and CPU required. 
        Environment variables. 
        Networking Information. 

        Task Role is required to perform the tasks. 

        Container Definitions: 
            Image, port 

        ECS -> Task Definition -> EC2 -> 
            Task Role - Optional IAM role that tasks can use to make API requests to authorized AWS services. Create an Amazon Elastic Container Service Task Role.
            Task execution role - This role is required by tasks to pull container images and publish container logs to Amazon CloudWatch on your behalf.
        
ECS Service: 
    It define how many tasks to run and how to run. 

    Deployment Type: Rolling update & Blue green powered by code deploy. 
    Task Placement: 
        AZ Balanced Spread 
        AZ Balanced Bin pack 
        BinPack 
        One Task Per host 
        Custom 

    ECS -> Cluster -> Services -> Create -> 


ECS Service with Load Balancers: 
    Application Load Balancer - Dynamic port forwarding - Allows to route to dynamic port. 
    Under Services -> ALB 


ECR Part 1:
    ECR is private docker image repository. 
    $(aws ecr get-login --no-include-email region us-east-1) 
    docker push 
    docker pull 

ECR Part 2: 
    Here updated the container image with private repository image. 


FARGATE: 
    In EC2 , requires to create cluster. 
    In fargate, no need of cluster which is serverless. 


ECS & Multi Docker beanstalk: 
    Elastic Beanstalk: 
        Single container    -> Run EC2 
        Multi Container     -> Run ECS 
    
    Requires Dockerrun.aws.json file under root of source code. 

ECS- IAM Roles: 
    ecsInstanceRole -> EC2 Instances for ECS Instances. 
    TaskRole -> Optional IAM role that tasks can use to make API requests to authorized AWS services. Create an Amazon Elastic Container Service Task Role.
    Task execution role - This role is required by tasks to pull container images and publish container logs to Amazon CloudWatch on your behalf.

ECS - AutoScaling: 
    Service -> COnfigure Auto Scaling. 
    Requires 2 auto scaling for ECS instances and Container instances. 

ECS - Cloud Watch Integrations: 
    Container Definition -> Integrate to cloud. -> Helps container send the log to cloud watch directly. 
    For ECS instances, install cloud watch log agent in ecs instances. 

ECS - Code Pipeline - CICD: 
    ecsworkshop.com 

------------------------------------------------------------------------------------------------------------------------------

OpsWorks: 
    It is configuration management with Chef and Puppet. 
    It helps to build highly dynamic application and propagate changes instantly. 
    It consists of: 
        OpsWorks Stacks -> Define, group, provision and operate application in AWS by using chef in local mode.
        OpsWorks for Chef Automate -> Create chef servers and use chef tooling to automate them. 
        OpsWork for Puppet Enterprise -> Create puppet server which monitor the infrastructure. 
    
    Stack is a set of layers, instances and related AWS resources whose configuration want to manage together. 

    Ops Work Stacks -> Stacks ->  Create Stack. 

    A layer is a blueprint for a set of Amazon EC2 instances. It specifies the instance's settings, associated resources, installed packages, profiles, and security groups. You can also add recipes to lifecycle events of your instances, for example: to set up, deploy, configure your instances, or discover your resources.

    Opsworks stack: 
        Elastic Loadbalancing Layer 
        Application server layer  -> Cookbook repository is used to provision. 
        Amazon RDS layer 
    
    Layers: 
        Auto healing enabled : yes 

    Instances: 
        Time based -> can select the time/schedule when to run. 
        Load based -> runs based on the load. 
        24/7  -> runs 24/7 

Life cycle events: 
    Setup       -> After instance startup and booting. 
    Configure   -> It occurs on ALL of instances. An instance enters or leaves online, Associate or Disassocite elastic IP address, Attach or Detach ELB from instance. 
    Deploy      -> It occurs when run deploy command. to deploy the application. 
    Undeploy    -> To remove the app from instances. 
    Shutdown    -> Shutdown happens before terminating. 

Auto healing & Cloud watch events: 
    If any failure, it will recreate the instances. 

------------------------------------------------------------------------------------------------------------------------------
                                MONITORING AND LOGGING
------------------------------------------------------------------------------------------------------------------------------

Cloud Trail: 
    It continuosly logs AWS accounts activity. 
    It tracks API call details. 
    It send to S3 bucket, SNS Notification or Send to Cloudwatch logs. 

    Cloud Trail -> Name, S3 bucket, 
        Log file validation - To verify whether log file modified/deleted. 
        SNS Notification delivery -> To notify in SNS for each log delivered. 
        Event Type: 
            Management Events -> Capture management operations performed on AWS resources. 
            Data Events -> Log the resource operations performed on/with in resources. 
            Insight Events -> Identity unusual activity, errors or user behaviour. 

Cloud Trail - Log Integrity: 
    Log file validation - To verify whether log file modified/deleted. 
    aws cloud-trail validate logs -> Command 
    Digest file deliver every 1 hour and then run the validate log command. 

Cloud Trail - Cross Account Logging: 
    To allow multiple account to send cloud trail logs to S3 bucket. 
        1. Requires to update the S3 bucket policy to allow other account. 
        2. In other accounts, choose existing bucket name which created in another account. 

------------------------------------------------------------------------------------------------------------------------------

Kinesis - Data Streams Overview: 
    It is alternative to apache kafka. 
    It is used for real time stream. 
    It consists of: 
        Kinesis stream      -> Low latency streaming ingest atscale. 
        Kinesis Analytics   -> Perform real time analytics on stream. 
        Kinesis Firehose    -> Load streams into S3, Redshift, Elasticsearch 

        Producers -> SHARDS -> Consumers
    Multiple application can consume the same stream. 
    Data retention is 1 day upto 7 days. 
    Records are ordered per shard. 
    Producer: 
        It consists of Record Key and Data blob and sequence nbr. 
        1 mb/s or 1000 messages per second write. 
    Consumer: 
        2 mb/s read. 
        5 api calls per second per shard. 
    
    Kinesis SDK, Agent, Cloud Watch Logs -> Kinesis Stream 
    AWS Kinesis KCL: (Kinesis Client Library): 
        It uses dynamodb to checkpoint offsets. 
        It uses dynamodb to track other workers and share work among shards. 
        great for reading in distributed manner. 
    
Kinesis Alalytics & Firehose: 
    Firehose: 
        It is fully managed and in near real time (60 sec latency)
        It loads data to S3, Redshift, Splunk, ELasticsearch. 
        Data transformation through AWS Lambda. 
        Supports compression when target is S3. 
    Analytics: 
        It perform real time analytics using SQL. 

    Kinesis -> Kinesis Data Firehose -> Create delivery stream -> Select source, destination

------------------------------------------------------------------------------------------------------------------------------

Cloud Watch Metrics:

    It is to track metrics like cpu for all aws services. 
    Detailed monitoring - Metrics comes in every 1 minute and to pay. 
    Basic monitoring - Every 5 minutes and no charges. 
    Upto 15 months maximum to check the metrics. 

        < 60 seconds data available for 3 hours. 
        1 minute data available for 15 days. 
        5 minutes data available for 63 days 
        1 hour data available for 15 months. 

    Cloud watch -> Metrics 

    Cloud watch agent used to get EC2- RAM details. 
    In EBS, How much space left can get using cloud watch agent. 
    In ASG, Group Metrics not enabled by default. 

Cloud Watch Custom Metrics: 
    Standard Resolution     -> Granularity of 1 minute. 
        High Resolution         -> Granularity of 1 sec. 
        Can publish custom metrics using API or CLI using putmetricdata call. 

        aws cloudwatch put-metric-data --metric-name Buffers --namespace MyNameSpace --unit Bytes --value 231434333 --dimensions InstanceId=1-23456789,InstanceType=m1.small

        aws cloudwatch get-metric-statistics --metric-name Buffers --namespace MyNameSpace --dimensions Name=InstanceId,Value=1-23456789 Name=InstanceType,Value=m1.small --start-time 2021-12-23T04:00:00Z --end-time 2021-12-23T07:00:00Z --statistics Average --period 60

Cloud Watch Metrics - Exports: 

    aws cloudwatch get-metric-statistics --metric-name Buffers --namespace MyNameSpace --dimensions Name=InstanceId,Value=1-23456789 Name=InstanceType,Value=m1.small --start-time 2021-12-23T04:00:00Z --end-time 2021-12-23T07:00:00Z --statistics Average --period 60

    Use cloud watch events to trigger lambda for every specific schedule. Lambda is used to export metrics to S3. 

------------------------------------------------------------------------------------------------------------------------------

Cloud Watch Alarms: 

    Based on metrics, it trigger the alarm. 
    Cannot combine multiple metrics for one alarm. One metric for one alarm. 

    ALARM           -> Outside of threshold. 
    INSUFFICIENT    -> Insufficient data. 
    OK              -> Within Threshold. 

    Cloud Watch -> Alarm -> Create Alarm 

    Send notification to SNS. 

    In Alarm: 
        Auto scaling actions
        EC2 Action 
        Systems Manager Action -> Create Incident or OpsItem 

Cloud Watch Alarm - Billing Alarm: 

    Cloud watch -> Alarms -> Billing (Metric Name - EstimatedCharges)
    It can be at global level or service level. 

------------------------------------------------------------------------------------------------------------------------------

Cloud watch Logs - Overview: 

    Cloud Watch -> Logs -> Log Group -> Log Streams -> Logs 
    /aws/service-name/  -> Managed by AWS 
    Expire Events After Option -> To set expiry date. 
    Can create or delete log stream. 

Cloud Watch Unified Agent: (Send metrics and logs) 

    https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/UseCloudWatchUnifiedAgent.html
    https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html

    It uses the ssm parameter store to keep the config details. 

Cloud Watch Logs - Metric Filters & Alarms: 
    Cloud watch -> Log group -> Create metric filter : 
        Define pattern - like Error , Info etc 
        Assign a metric 

Cloud Watch Logs - Export to S3: 
    Cloud watch -> Logs -> Actions -> Export data to Amazon S3. 
    It is done manually. 

Cloud Watch Logs - Subscriptions: 
    Cloud watch -> Logs -> Actions -> Subscriptions -> : 
        Kinesis, Kinesis Firehose, Lambda, Amazon opensearch service. 

------------------------------------------------------------------------------------------------------------------------------

Cloud Watch Events (Amazon event bridge) : 

    1. create a rule (Source or schedule)
    2. select a target 

Cloud watch event - Integration with cloud trail: 
    AWS API Call via Cloud Trail available for all services. 
    Can give our own API call. 
    This helps if some API action is not listed as part of AWS services. 

Cloud watch events vs S3 Events: 
    S3 -> bucket -> Properties -> Event Notifications 
        Event types can be Object creation, removal, restore, ACL, Tagging, Replication, Life cycle, Intelligent Tiering. 
        Destination can be Lambda, SNS, SQS 
        Here little event types are supported but in event bridge have support multiple and more events. 

------------------------------------------------------------------------------------------------------------------------------

Cloud Watch Dashboards:
    Cloud watch -> DAshboard -> Create dashboard 
        Data source can be logs or Metrics. 
        Correlate the data using dashboard. 
        It is multi region. 

------------------------------------------------------------------------------------------------------------------------------

X-Ray Overview: 
    It is like app dynamics. 
    For distributed application, it gives service map. 
    It helps to analyze and debug applications. 
    It used for debugging, tracing and service map. 


------------------------------------------------------------------------------------------------------------------------------

Amazon ES - ELK (Elastic Search + Kibana + Logstash):
    Now open search. 
    Cloud watch -> Subscription -> Open Search. 
    
------------------------------------------------------------------------------------------------------------------------------

Tagging in AWS:
    Cost tracking, Automation, Security, Organization, 

------------------------------------------------------------------------------------------------------------------------------
                                POLICIES AND STANDARD AUTOMATION
------------------------------------------------------------------------------------------------------------------------------

SSM Overview & Quick Setup: 

    It helps manage EC2 & On-Prem systems at scale. 
    Get operational insights about state of infrastructure. 
    Easily detect problems. 
    PATCHING automation for enhanced compliance. 
    Works for both windows and linux. 
    Integrated with AWS Config and Cloudwatch metrics/dashboard. 

It consists of: 
    Resource Groups 
    Insights 
    Parameter store 
    Automation 
    Run COmmand 
    Session Manager 
    Patch Manager 
    Maintenance windows 
    State Manager - define and maintain configuration of OS and applications. 

    Need to instal SSM agent to instances. For few instances, default installed. 
    Systems Manager -> Quick Setup -> Select instances -> Create 

SSM - EC2 Setup: 
    https://docs.aws.amazon.com/systems-manager/latest/userguide/agent-install-al.html
    sudo systemctl start amazon-ssm-agent 
    sudo systemctl status amazon-ssm-agent 

SSM On-Premise Setup: 
    Hybrid activation involves non AWS machines like On-Prem, other machines can integrate with SSM.
    There is NO IAM role tagged to simulate as on-prem 
    https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-managed-linux.html
    It requires activation code & activation id: 
        SSM -> Node Management -> Hybrid Activations -> Create an activation -> 

    Activation Code   
    Activation ID   63edbf71-40fd-4771-b2cd-bd138f5ee8ec

    sudo /snap/amazon-ssm-agent/current/amazon-ssm-agent -register -code "xxxxxx" -id "63edbf71-40fd-4771-b2cd-bd138f5ee8ec" -region "us-east-1" 

    For aws managed instances, tagged with i- 
    for other resources like on-prem, tagged with mi- 

AWS Resource Groups: 
    It helps to find and group AWS resources using queries. 
    AWS Resource groups -> Create a resource group : 
        Group by Tags, Cloud Formation stack 

SSM Run Command: 
    It used to run a command on specific instances. 
    It is done by using documents. 

    SSM -> Documents -> create document (Command/Session or Automation) -> JSON Document. 
    SSM -> RUN Command -> Select Document -> Select Instances -> Run 
    AWS Command Line Interface Command -> It gives CLI command which can be used to execute separately. 

SSM Parameter Store: 
    SSM -> Parameter store. 
    Standard  (< 10k & 4kb limit) & Advanced Type (> 10k & 8 kb limit)
    Type - String, StringList, SecureString (Encrypt using KMS key). 
    /myapp/dev/environment 

    aws ssm get-parameters-by-path 

SSM Patch Manager: 
    SSM -> Patch Manager -> Patch Now 
    Available for Install type only not for scan. Using life cycle hooks, can run systems manager documents during Before installation, After installation and On Exit. 

SSM Maintenance Window: 
    Schedule and implement maintenance tasks across fleet. 
    It can be tagged to : 
        Register Targets 
        Register run command 
        Register Automation task 
        Register Lambda task 
        Register step function task 

SSM Inventory: 
    It has list of instances track by SSM
    It helps to check list of installed apps on instances. 

SSM Automations: 
    1. Create the document. 
    2. Use that document in Automation creation. 

------------------------------------------------------------------------------------------------------------------------------

AWS Config: 
    It provides a detailed view of resources associated with AWS account, including how they are configured , how they are related to one another and how the configurations and their relationships have changed over time. 
    It gives summarized view of AWS and Non AWS resources and compliance status of rules and resources in each AWS region. 

    AWS Config -> Select resources -> Tag RULES -> Create 

    Conformance Pack -> It is collection of AWS Config rules and remediation actions that can be deployed and monitor as single entity. 

    Aggregator -> It collects AWS Config data from multiple accounts and region. Use an aggregator to view the resource configuration and compliance data recorded. 

    Timeline 
    Interaction with cloud trail to get who and what changed. 

Config Rules: 
    AWS Config -> Rules -> Add rule (AWS Managed or Custom Lambda based rule)
    Trigger type - Configuration changes or Periodic 

Config - Automations: 
    SNS Notifications 
    Amazon cloudwatch event rule. 
    Remediation capability with Rules for any compliant issue. 

Config - Multi Account: 
    Aggregator used for multi account and multi region to collect config data. 
    Authorization allows to give permission for the account and region list to process aggragator. 

------------------------------------------------------------------------------------------------------------------------------

Service Catalog: 

    It allows organisations to create and manage catalog of IT services that are approved for use on AWS. 
    It helps to create self service portal which have pre approved services. 
    It helps new AWS users. 
    Product -> Portfolio - Teams (Collection of products) -> Access  (Admin Task)
    Product list -> Launch (User Tasks)
    Product are cloudformation templates. 

------------------------------------------------------------------------------------------------------------------------------
Inspector: 

    It enables to analyze behaviour of AWS resources and helps to identify potential security issues. 
    It is vulnerability management service that helps scans workloads for software vulnarabilities and unintended network exposure. 
    Network assessment (requires no agent), Host assessment (requires agent to install).
    It uses templates to check the vulnarabilities. 

    Inspector -> Enable inspector. 
    It can be integrate with cloud watch events. 
    Can send notification after the events like run start, finished, report etc. 

------------------------------------------------------------------------------------------------------------------------------

EC2 Instance Compliance: 

    AWS Config: 
        It ensures proper configuration using Rules. 
        For ex: Security group is tagged. 
        For Audit and Compliance over time. 
    
    Inspector: 
        Use inspector agent to scan vulnarabilities check for host. 
        For network scan. 
        Require instance to be launch in advance. 
    
    Systems Manager: 
        Run automations, run command, patch, inventory. 
    
    Service Catalog: 
        Restrict user with in product (services). 
    
    Configuration Management: 
        SSM, OpsWorks, Ansible, Chef, Puppet, User data. 
        
------------------------------------------------------------------------------------------------------------------------------

Health - Service Health Dashboard & Personal Health: 
    Service Health DAshbord is global and get as RSS feed. 
    Personal Health Dashboard - It can integrate with cloud watch events 

------------------------------------------------------------------------------------------------------------------------------

Trusted Advisor: 
    It provides recommendations that helps to follow the aws best practices. 
    It evaluates account using Checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs and monitor service quotos. 

    It can integrate with cloud watch events. 
    Send to slack channel using lambda thorugh health service 
    Trusted advisor -> Event bridge -> Lambda (low utilization, snapshot backup, exposed accessed key )

    Manual refresh for every 5 mins. 
    API - refresh-trusted-advisor-check 


------------------------------------------------------------------------------------------------------------------------------

Guard-Duty: 

    Intelligent threat discovery to protect aws account. 
    uses machine learning, anamoly detection, 3rd party data. 
    Checks cloud trail logs, vpc flow logs, DNS logs. 
    Notifies in case of failure.
    Integrate with event bridge. 

------------------------------------------------------------------------------------------------------------------------------

Macie: 

    It is data security and data privacy service that helps to discover, monitor and protect sensitive data. 
    It helps to detect any PII data. 
    Macie -> Enable Macie 

------------------------------------------------------------------------------------------------------------------------------

Secrets Manager: 

    Easily rotate, manage and retrieve secrets throughout their lifecycle. 
    Replica Secret - Helps to replicate secret in other region. 
    Automatic rotation enable to rotate automatically using lambda function. 

------------------------------------------------------------------------------------------------------------------------------

License Manager: 

    It makes easier to manage software license from vendors such as microsoft, oracle etc. 
    It helps to create customize licensing rules.
    Also can use to easily distribute and track license. 
    Map license to Resources/Instances. 

------------------------------------------------------------------------------------------------------------------------------

Cost Allocation Tags: 
    Tags used to group related resources. 
    AWS Generated Cost Allocation Tags (aws:)
    User Tags (user: )
    Takes 24 hours to reflect in billing. 
    Account -> Billing Dashboard

------------------------------------------------------------------------------------------------------------------------------

Data Protection & Network Protection in AWS: 

    TLS for in-transit encryption. 
    ACM to manage SSL/TLS certificate. 
    Load balancer: 
        ELB, ALB, NLB provide SSL termination. 
    Cloudfront with SSL 
    All AWS services expose HTTPS endpoint. 

    S3 Encryption: 
        SSE-S3      -> S3 encryption 
        SSE-KMS     -> Use KMS key 
        SSE-C       -> Customer provided key. 
        Client side encryption -> Send encrypted object. 
        x-amz-server-side-encryption -> to enforce encryption. 
    
    PHI - protected health information
    PII - personally identifiable information 

    Direct Connect, VPN uses IPSec.

    WAF - web security rule against exploits. 

------------------------------------------------------------------------------------------------------------------------------
            INCIDENT, EVENT RESPONSE, HIGH AVAILABILITY, FAULT TOLERANCE AND DISASTER RECOVERY 
------------------------------------------------------------------------------------------------------------------------------

Launch configuration or Template is used to define the configuration template. 

Auto Scaling Group - From Launch Configuration: 

    EC2 -> Auto Scaling Group -> Select Launch Configuration, Load balancer, Desired, Min & Max size, Scaling policy.

    ASG -> Launch Configuration: 
        Name, AMI, Instance Type, Volume, Security group, Key Pair 


Auto Scaling Group - From Launch Template: 
    It is advanced way of defining the template. 
    It can be versioned. 
    Can create a new template from existing source template. 
    From launch template, can launch an instance, create auto scaling group, create spot fleet. 
    It allows to use mix of On demand + Spot purchasing options. 

    EC2 -> Auto Scaling Group -> Select Launch Template, Load balancer, Desired, Min & Max size, Scaling policy.

    Instances -> Launch Template: 
        Name, AMI, Instance Type, Volume, Security group, Key Pair 

ASG - Scheduled Actions: 
    If know the pattern or load in advance, can schedule to run the ASG in specific date or recurrence. 
    It helps to provide min,max and desired capacity to run on specific timing or recurrence like CRON. 
    EC2 -> ASG -> Automatic Scaling -> Scheduled Actions. 
    
ASG - Scaling Policies: 

    EC2 -> ASG -> Automatic Scaling -> Dynamic Scaling Policy, Predictive Scaling Policy. 
    Dynamic Scaling Policy: 
        Target tracking -> Target based on metric like CPU uitlization. 
        Step tracking -> Add step based on Cloud watch alarm. 
        Simple tracking -> Add capacity based on Cloud watch alarm. 

    Predictive Scaling Policy - Predictive scaling builds a forecast based on historical data and scales out capacity in advance of forecasted hourly load, so that new instances are ready to handle traffic when the load arrives.

    Default Cooldown - Default 300s. No of seconds after scaling activity completes and before new instance begins. 
    Seconds to WarmUp after scaling.
    Disable scale in while scale out. 

ASG - ALB Integration: 
    1. Created the ALB. 
    2. tag the ALB to ASG. 
    Slow start duration - It takes some time for new scaled in instances. 

ASG - HTTPS on ALB: 
    ALB -> Listener -> HTTPS to be enabled. 
    It requires SSL certificates and can be imported from IAM, ACM or Import manually to ACM/IAM.
    For redirection from HTTP to HTTPS -> Listener -> Redirect to HTTPS 

ASG - Suspending Process and Troubleshooting: 

    Use standby feature instead of suspend feature to troubleshoot and reboot an instance. 
    Scaling Process Types: 
        Launch 
        Terminate 
        AddToLoadBalancer -> will not add instances to load balancer. After suspend, require to manually to register instance to target group. 
        AlarmNotification 
        AZRebalance 
        HealthCheck 
        InstanceRefresh 
        ReplaceUnhealthy 
        ScheduledActions 
    
    All these process types can be suspended and resumed independently. 

    EC2 -> ASG -> Details -> Advanced Configurations -> Suspend process. 

    Detach - Create replacing instances. 
    Standby - Doesnot create replacing instances. 
    ScaleIn protection - prevent instances from terminating. 

ASG - Lifecycle Hooks: 

    It is having hooks gives ability to perform custom actions based on event occurs. 

    Scale out: 
        Pending     -> Pending-wait , Pending-proceed 
        Inservice 
    Scale In: 
        Terminating -> Terminating-wait, Terminating-proceed  
        Terminated 

    EC2 -> ASG -> Instance Management -> Lifecycle hooks 

    After timeout, continue or abondon. 
    
    It trigger SNS, SQS and Lambda through Events bridge. 

    Warm Pool - Decrease scale-out latency by pre-initializing EC2 instances and save money by reducing the number of continuously running instances.

    aws autoscaling-complete-lifecycle-action command used to complete the hooks. 

    Lambda triggers SSM document to run user scripts in ec2. 


ASG - Termination Policies: 

    Allocation strategy 
    Closest to next hour instances. 
    Default 
    Newest instance 
    Oldest instance 
    Oldest launch configuration 
    Oldest launch template 
    custom termination policy 

    Termination Priority: 
        1. Which AZ has more instances. 
        2. Allocation strategy for on-demand and spot instances. 
        3. Any using oldest launch configuration or template. 
        4. Instances closest to next billing hour. 

ASG - Integrity with SQS: 

    SQS Queue -> Application -> Emits custom metric -> Amazon cloud watch -> Auto scaling event -> Change capacity of instances. 
    can do API call to enable scale in protection. 

ASG - Monitoring: 

    EC2 -> ASG -> Monitoring -> Cloud watch 
    EC2 -> ASG -> Activity -> Activity Notification 

ASG - CloudFormation Creation Policy: 

    Type: AWS::AutoScaling::AutoScalingGroup 
    Use CreationPolicy to wait to receive signal from instances. 

ASG - CloudFormation Update Policy: 

    If updatepolicy is not used, then existing instances will not be updated. 
    Can update with same template and will show change set. 
    UPdatePolicy - AutoScalingReplacingUpdate, AutoScalingRollingUpdate, AutoScalingScheduledAction. 

ASG - Code Deploy Integration: 

    Code Deploy supports: 
        Auto scaling group -> Select the ASG 
        EC2/On prem 

    Code deploy will automatically deploy application revision to ASG new instances. 

    While desired capacity changed, it takes the last versions. Incase if any deployment, unless success takes the last version. use suspend feature or standby. 

ASG - Deployment Strategies: 

    In Place    -> Update on same instance  
    Rolling     -> Create new instance 
    Replace     -> Create new set of instances. 
    Blue Green  -> use load balancer, route 53, shift traffic to new instances. 

------------------------------------------------------------------------------------------------------------------------------

DynamoDB: 

    Partition Key, Sort Key   -> Primary key 
    Secondary Index: 
        Local secondary Index (LSI) - Cannot create once table created and partition key is same. RCU & WCU inherit from table. 
        Global secondary Index (GSI) - can create after table created and partition key & sort key can be different. It requires RCU & WCU. 
        Read/Write Capacity Unit: 
            Ondemand or Provisioned 

    DAX - DynamoDB Accelarator -> Used for Cache and require to provision. 

    Stream - used to stream the data (old or new). It requires to enable in dynamodb. From lambda, trigger the dynamodb stream. 

    GLobal tables enables to use dynamodb as fully managed, multi region, multi master database. Ensure table is empty and stream is enabled. It helps to create replica table in another region. 
    In Global tables, if updated in one region, it will reflect in another region. 

    No more than 2 processes should be reading from the same stream shard at same time. otherwise can result in throttling. Because in underlying uses kinesis stream which have 2 mb/s read.

    TTL (Time To Live) (Use key as ttl and value as epoch time, particular key will be expired in given time). It requires to enable TTL and give the TTL attribute like ttl (which is used in key).

    Big File  -> S3 -> Lambda -> Dynamodb (writes metadata to dynamodb)
    Dynamodb -> Dynamodb stream -> Lambda function -> Amazon ES 

------------------------------------------------------------------------------------------------------------------------------ 
S3: 

    S3 Events available to send notification sns, sqs, lambda. 
    Requires to enable cloud trail to work with cloud watch events. 
    Cross region replication -> to replicate in another region with same or different account. 
    Same region replication.
    Life cycle policy - Rules to transition object to other like IA, Glacier, Intelligent tiering. 
    Encryption - None, AES 256 (s3), KMS, SSE - C, Client side encryption. 
    Server Access Log - can enable logging. 

------------------------------------------------------------------------------------------------------------------------------ 

Multi AZ Overview: 

    Services where multi AZ must be enabled manually: 
        EFS, ELB, ASG, EB, 
    RDS, ELastic Cache (Standby DB)

    Services multi AZ implicitly: 
        S3 (except one zone IA)
        DynamoDB 
        All of fully managed services. 
    
    in EBS multi AZ process, from ASG, use hooks to take snapshot of EBS, Create EBS from snapshot and use that EBS for instances.

------------------------------------------------------------------------------------------------------------------------------ 

Multi Region - Overview: 

    Dynamodb global tables (active active).
    AWS Config aggregators (with multi account and multi region)
    RDS Cross Region read replicas (Used for DR and Read)
    Aurora Database (One is for master and other for read/DR)
    EBS Volumue Snapshots, AMI, RDS snapshots can copy to other region. 
    VPC Peering allow traffic between regions. 
    Route 53 is global network for DNS servers. 
    S3 cross region replication. 
    Cloud front is CDN at edge locations. 
    Lambda@Edge is used in cloudfront at edge locations. 

Multi Region with Route 53: 

    Route 53  ->    Loadbalancer + AWS Services  (Region 1)
                    Loadbalancer + AWS Services  (Region 2) 

        Based on health check, route traffic to different region. 

------------------------------------------------------------------------------------------------------------------------------ 

Multi Region - Cloud Formation - Stacksets: 

    Stacksets is used to create/update/delete stacks in multi account and multi region with single operation. 
    Enabled AWS COnfig in other regions using cloud formation stack. 


 ------------------------------------------------------------------------------------------------------------------------------ 
Multi Region - Code Pipeline: 

    In pipeline, during code deploy, can select other region.     

------------------------------------------------------------------------------------------------------------------------------ 

Disaster Recovery - Overview: 

    Hybrid -> On prem to AWS Cloud 
    Cloud  -> AWS Region A to Region B 

    RPO -> Recovert Point Objective -> how long data loss is allowed. how frequency backup is taking. 
    RTO -> Recovery Time Objective  -> How long failure/downtime is allowed. 

Strategies: 
    Backup & Restore: 
        High RPO , Storage Gateway , Snapshots
        It is Cheap. 
        High RTO 

    Pilot Light: 
        Core component are running always. 
        Just require to add on other non critical components. 
        Faster than backup. 

    Warm Standby:
        Full system is up and running at lower spec. 
        upon disaster, scale to production load. 
        
    Hot Site / Multi site approach: 
        It is expensive and full production scale is running in another region. 
        if disaster, switch to other site. 

------------------------------------------------------------------------------------------------------------------------------ 

Disaster Recovery - Devops Checklist: 

    Is AMI copied and stored in parameter store. 
    RPO & RTO defined. 
    Cloudformation stackset works in another region. 
    Route 53 works correctly. 
    Is all data backed up. 
    RDS read replication enabled. 

    Backups: 
        AWS Backup with EFS. 
        Route 53 backup - ListResourceRecordSets for export. Script to import recordsets in to another DNS. 
        Elastic beanstalk - use saved configuration 

------------------------------------------------------------------------------------------------------------------------------ 

On Premise Strategies with AWS: 

    Ability to download linux 2 as .iso image and allow to run in on-prem. 
    VM Import/Export - Migrate application to EC2, Export the VM from EC2 to On-prem. 
    Application Discovery Services - Gather information about on-prem and plan a migration. 
    Database Migration service - Migrate database from AWS <> On-prem, On-prem <> AWS 
    Server Migration Service - Migration of On-prem live servers to AWS.

------------------------------------------------------------------------------------------------------------------------------ 

Multi Account - AWS Organization: 

    Master account and cant change. 
    Member accounts part of one organization. Requires invite to join in organization. 
    Consolidated billing. 
    Pricing benefits from aggregated usage. 

    Organizational Unit (OU) - can be based on region, business etc. 

    Service Control Policies (SCP) - Policy applied at member accounts. 

    Cross account requires IAM trust. IAM Roles can be assumed in cross account using STS. 

    Cross account invocation of code deploy using cloud formation stack. 

    AWS Config aggregators. 

    Cloud Formation stack sets. 

    Event bus. 

    Centralized logging: Log group -> LogDestination create using CLI, it connect with kinesis firehose and connect to S3.

------------------------------------------------------------------------------------------------------------------------------ 

