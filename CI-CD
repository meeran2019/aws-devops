
------------------------------------------------------------------------------------------------------------------------------
                                CI-CD OVERVIEW
------------------------------------------------------------------------------------------------------------------------------

Continuos Integration:
    Push the code to Code commit/ github etc.
    Once code pushed, build/test server process the code. 

Continuos Delivery/Deployment:
    Continuos delivery involves manual step. 
    Continuos deployment is fully automated. 

Code Repository:AWS Code Commit, Git Hub
Build & Test:   AWS Code Build , Jenkins, Bamboo 
Provision:      AWS Elastic Bean Stalk 
Deploy:         AWS Code Deploy (User managed EC2, On Prem, Lambda, ECS)
Orchestrate:    AWS Code Pipeline 

-------------------------------------------------------------------------------------------------------------------------------

Code Commit: 
    It is private fully managed AWS git repository. 
    It is like Git Hub, Bit bucket.
    Code is encrypted and store under the account. 

Code Commit - Create First Repository & HTTPS Configuration:
    It can be connected through HTTPS & SSH Connection.
    SSH is not enabled for root account. 
    Under IAM User -> security credentials -> SSH Keys or HTTPS Git credential for Code Commit. 
    These credentials are used for push or pull from repository. 

    Code Commit -> Create Repository -> (Repository Name) -> Done. 
    Code Commit -> Repository -> View Repository 
    Clone URL gives the repository URL which will copied. 

Code Commit - Clone, Add, Commit, Push:
    git status          -> Shows status whether tracked or untracked. 
    git add .           -> Add files to staging area. 
    git commit -m 'msg' -> Commit the change in local git. 
    git push            -> Push to the repository. During push, asks for username and password. 

Code Commit - Branches and Pull Requests:
    git checkout -b branch_Name     -> It creates new branch and switch to new branch. 
    git branch branch_Name          -> Create a new branch. 
    git checkout branch_Name        -> Switch to new branch. 

    1. Created the new branch and modified with new feature. 
    2. Create Pull request (Merge) -> Merge the new feature branch to master branch. 
    3. In create pull request, select source as 'new feature branch' and destination as 'master' branch. 
    4. If no merge conflicts, will allow to create pull request. 
    5. From pull request section, Reviewer click 'Merge' to accept pull request. (Here have default option to delete the new-feature branch if required)  

Code Commit - Securing the repository and Branches: 
    It can be done by creating the policy to explicit deny to secure master branch. 
    Available in : https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-conditional-branch.html
    Note: In below policy given for main branch only, Need to add master branch. 

                {
                "Version": "2012-10-17",
                "Statement": [
                    {
                        "Effect": "Deny",
                        "Action": [
                            "codecommit:GitPush",
                            "codecommit:DeleteBranch",
                            "codecommit:PutFile",
                            "codecommit:MergeBranchesByFastForward",
                            "codecommit:MergeBranchesBySquash",
                            "codecommit:MergeBranchesByThreeWay",
                            "codecommit:MergePullRequestByFastForward",
                            "codecommit:MergePullRequestBySquash",
                            "codecommit:MergePullRequestByThreeWay"
                        ],
                        "Resource": "arn:aws:codecommit:us-east-1:*:*",
                        "Condition": {
                            "StringEqualsIfExists": {
                                "codecommit:References": [
                                    "refs/heads/main",
                                    "refs/heads/master"
                                ]
                            },
                            "Null": {
                                "codecommit:References": "false"
                            }
                        }
                    }
                ]
            }


    It failed with below error, so not allowing to push to master branch:

        error: remote unpack failed: internal error
        To https://git-codecommit.us-east-1.amazonaws.com/v1/repos/devops-code-commit-demo
        ! [remote rejected] master -> master (unpacker error)
        error: failed to push some refs to 'https://git-codecommit.us-east-1.amazonaws.com/v1/repos/devops-code-commit-demo'


    Code Commit - Triggers and Notifications:
        Notification - If any process like push, pull etc, it can be notified by using SNS. 
        Trigger - Same as notification but inaddition to SNS it allows to trigger Lambda. 
        Cloud watch Event - It can also done here by select source as code commit and target as any like SNS, Lambda etc. 

        Code Commit -> Settings -> Notifications Tab:
            Name, Events, SNS Target to select. 

        Code Commit -> Settings -> Trigger Tab: 
            Can select SNS or Lambda. 
        
        Event Bridge -> Rules -> Create Rule: 
            Name, Service Provider, Target 
    
    Code Commit - AWS Lambda: 
        https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html
        
        1. Created the lambda function with trigger as code commit. 
        2. If any change in events in code commit, triggered the lambda. 
        3. Verified in cloud watch logs. 


-------------------------------------------------------------------------------------------------------------------------------

Code Build:
    It is fully managed build service like Jenkins. 
    Leverages Docker under the hood. 
    Source code from S3, Code Commit, Code Pipeline etc. 
    Build instruction defined in buildspec.yml file. 
    Output to S3 and cloud watch logs. 
    Can integrate with Event bridge to detect failures. 
    SNS & Lambda integrations. 

Code Build - First Build: 
    code build -> build projects -> 
    Sources - No sources/S3/Code Commit/Git hub/Bit bucket. 
    Reference Type can be Branch/Git tag/ Commit ID 
    Once build created, click 'start build' to run the build.                                                   


Code Build - buildspec.yml file: 
    It can be placed in S3 or root of source directory.


        version: 0.2                    # represent buildspec version. recommended to use 0.2 
        run-as: Linux-user-name         # optional specific to linux. 
        env:
        shell: shell-tag
        variables:
            key: "value"
            key: "value"
        parameter-store:
            key: "value"
            key: "value"
        exported-variables:
            - variable
            - variable
        secrets-manager:
            key: secret-id:json-key:version-stage:version-id
        git-credential-helper: no | yes

        proxy:
        upload-artifacts: no | yes
        logs: no | yes

        batch:
        fast-fail: false | true
        # build-list:
        # build-matrix:
        # build-graph:
                
        phases:
        install:
            run-as: Linux-user-name
            on-failure: ABORT | CONTINUE
            runtime-versions:
            runtime: version
            runtime: version
            commands:
            - command
            - command
            finally:                # this will be executed if previous is failed or success. 
            - command
            - command
        pre_build:
            run-as: Linux-user-name
            on-failure: ABORT | CONTINUE
            commands:
            - command
            - command
            finally:
            - command
            - command
        build:
            run-as: Linux-user-name
            on-failure: ABORT | CONTINUE
            commands:
            - command
            - command
            finally:
            - command
            - command
        post_build:
            run-as: Linux-user-name
            on-failure: ABORT | CONTINUE
            commands:
            - command
            - command
            finally:
            - command
            - command
        reports:
        report-group-name-or-arn:
            files:
            - location
            - location
            base-directory: location
            discard-paths: no | yes
            file-format: report-format
        artifacts:              
        files:
            - location
            - location
        name: artifact-name
        discard-paths: no | yes
        base-directory: location
        exclude-paths: excluded paths
        enable-symlinks: no | yes
        s3-prefix: prefix
        secondary-artifacts:
            artifactIdentifier:
            files:
                - location
                - location
            name: secondary-artifact-name
            discard-paths: no | yes
            base-directory: location
            artifactIdentifier:
            files:
                - location
                - location
            discard-paths: no | yes
            base-directory: location
        cache:
        paths:
            - path
            - path


Code Build - Docker, ECR & buildspec.yml file: 
    Code build used to build docker image and push to ECR. 
    
    1. Created the Dockerfile and buildspec.yml file. 
    2. Docker File: 
        FROM node:12-alpine
        RUN apk add --no-cache python3 g++ make
        WORKDIR /app
        COPY . .
        RUN yarn install --production
        CMD ["node", "src/index.js"]
    3. buildspec.yml: 
        version: 0.2
        phases:
        pre_build:
            commands:
            - echo Logging in to Amazon ECR...
            - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com
        build:
            commands:
            - echo Build started on `date`
            - echo Building the Docker image...          
            - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .
            - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG      
        post_build:
            commands:
            - echo Build completed on `date`
            - echo Pushing the Docker image...
            - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG

    4. zip myzip.zip Dockerfile buildspec.yml 
    5. Use the environemental variables IMAGE_REPO_NAME=ECS Repository Name , IMAGE_TAG=latest version. 
    6. After build completed, ECR -> Repositories -> Images. 


Code Build - Environmental Variable and Parameter Store:
    https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-env-vars.html
    Link has list of environmental variables can use. 
    printenv in buildspec.yml (under commands) file prints all environmental variables. 
    environment variables can be given in buildspec file or override in console. 
    Parameters can be place from Systems Manager Parameter Store or Secrets Manager. 

Code Build - Artifacts & S3: 
    1. Include the artifacts in buildspec.yaml file. 
    2. In code build console, update to include the artifacts by giving s3.  
    3. In code build, start the build. 
    4. Artifacts created in S3. 

Code Build - Cloud watch Events, Cloudwatch Logs, Cloud watch metrics & Triggers:
    Code build -> View Projects -> edit -> Logs: 
        can select the cloudwatch logs and also s3 bucket to store logs. 
    
    Cloud watch -> Metrics -> Code build -> Select by Project or Account level. 
    Metrics can export to dashboard. 

    Cloud watch -> Events -> Rules: (Now changed to Amazon event bridge): 
        Event bridge -> Name, Event Pattern or Schedule (use cron to run at specific time) -> create Rule. 
    
Code Build - Validating code commit Pull Requests: 
    https://aws.amazon.com/blogs/devops/validating-aws-codecommit-pull-requests-with-aws-codebuild-and-aws-lambda/


-------------------------------------------------------------------------------------------------------------------------------

Code Deploy:
    EC2 or On-prem machine must running code deploy agent. 
    Agent continuosly polls code deploy for work. 
    code deploy has appspec.yml file. 
    Application is pulled from s3 or git hub. 
    Ec2 instances are group by deployment group. 
    It can integrate with pipeline. 
    Blue-Green works only with EC2 (not on-prem) 
    It support for Lambda, EC2. 

    Code + appspec.yml to S3 <->(Pulled by agent) EC2+agent. So EC2 requires access to read from S3. 

Code Deploy - EC2 Setup: 
    1. Create EC2 instance with TAGS mandatory. Tags are used by deployment group in code deploy service. 
    2. Create a service role with S3 access which requires to pull code_appspec.yml file from S3. 
    3. Install the code deploy agent in EC2. Either after ssh or use in user data field. 
            sudo yum update -y
            sudo yum install -y ruby wget
            wget https://aws-codedeploy-eu-west-1.s3.eu-west-1.amazonaws.com/latest/install
            chmod +x ./install
            sudo ./install auto
            sudo service codedeploy-agent status

Code Deploy - Application, Deployment Groups & FIrst Deployment: 
    1. Created the role (Allow code deploy to access EC2).
    2. Created the application. 
    3. Create the deployment group: 
            Name, Deployment type (Inplace or Blue green), Deployment Settings (Allatonce, oneatatime, halfatatime)
    4. Upload the code + appspecfile in zip to S3. 
            aws deploy push --application-name CodeDeployDemo --s3-location s3://aws-devops-course-stephane/codedeploy-demo/app.zip --ignore-hidden-files --region eu-west-1 --profile aws-devops            
    5. Create deployment: 
            s3 bucket location which contain code and then create deployment. 

    Note: TO check whether code deploy agent is properly installed. If not installed, then deploy will timeout with error. 

Code Deploy - Deployment Groups: 
    Under application, can create multiple deployment group with different tags. 
    This helps to separate for dev, prod instances. 
    During deploy, can select the specific deployment group for deployment. 

Code Deploy - Deployment Group Configurations: 
    Deployment Type: 
        Inplace - In existing instances, update the applications. 
        Blue Green  - Create new instances through manually or AutoScaling options. 
    
    Deployment Configurations: 
        AllatOnce, OneataTime, HalfataTime, Manually create deployment configurations. 
    
    Loadbalancer: 
        For blue-green, it is mandatory. 
        For inplace, it is optional. 

Code Deploy - appspec.yml file: 

    https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html

    NOte: Since run as root, no need of sudo in scripts. 

    version: 0.0
    os: linux
    files:
    - source: /
        destination: /var/www/html/WordPress
    hooks:
    BeforeInstall:
        - location: scripts/install_dependencies.sh
        timeout: 300
        runas: root
    AfterInstall:
        - location: scripts/change_permissions.sh
        timeout: 300
        runas: root
    ApplicationStart:
        - location: scripts/start_server.sh
        - location: scripts/create_test_db.shhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html

    Hooks availabel different for , Inplace, Blue green original & replacement, Lambda  & ECS. 
    Few environmental variables are available like APPLICATION_NAME, DEPLOYMENT_ID
    
    if [ "$DEPLOYMENT_GROUP_NAME" == "Staging" ]
    then
        sed -i -e 's/Listen 80/Listen 9090/g' /etc/httpd/conf/httpd.conf
    fi
        timeout: 300
        runas: root
    ApplicationStop:
        - location: scripts/stop_server.sh
        timeout: 300
        runas: root
        

Code Deploy - Hooks & Environmental Variables: 
    https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html

    Hooks availabel different for , Inplace, Blue green original & replacement, Lambda  & ECS. 
    Few environmental variables are available like APPLICATION_NAME, DEPLOYMENT_ID, DEPLOYMENT_GROUP_NAME, DEPLOYMENT_GROUP_ID, LIFECYCLE_EVENT 
    
    if [ "$DEPLOYMENT_GROUP_NAME" == "Staging" ]
    then
        sed -i -e 's/Listen 80/Listen 9090/g' /etc/httpd/conf/httpd.conf
    fi


Code Deploy - Cloud watch, Alarm, Trigger:
    Code Deploy -> Deployment groups -> Advanced Option -> Trigger 
    Code Deploy -> Deployment groups -> Advanced Option -> Alarm 
    Event bridge -> Code build -> Target can be any. 

Code Deploy - Rollbacks: 
    Code Deploy -> Deployment groups -> Advanced Option -> Rollbacks. 
    1. Rollback when deployment fails. 
    2. Rollback when alarm threshold are met. (This can use to monitor cpu utiliation, if more than to rollback)

Code Deploy - On Premise Setup: 
    https://docs.aws.amazon.com/codedeploy/latest/userguide/instances-on-premises.html

    Step 1 – Configure each on-premises instance, register it with CodeDeploy, and then TAG it.
    Step 2 – Deploy application revisions to the on-premises instance.

    1. use IAM User ARN  for authenticate request. -> Best for smaller instances. 
    2. use IAM role ARN for authenticate request. -> This is secure and complex. 

Code Deploy - Deploy to Lambda: 
    https://docs.aws.amazon.com/codedeploy/latest/userguide/applications-create-lambda.html

    hooks: 
        beforeallowtraffic functionname
        afterallowtraffic functionname 
    
    1. Create a service role for code-deploy for lambda. 
    2. Create the application. 
    3. Creaet the deployment group. 
    4. Deployment type: 
            allataonce
            canary - shifts in two increment 
            linear - shifts in equal increment. 
    5. Hooks contains beforeallowtraffic & afterallowtraffic. 


-------------------------------------------------------------------------------------------------------------------------------

Code Pipeline: 
    It is CI-CD Orchestrate tool. 
    Source : S3, Code Commit, ECR, Git hub, Bitbucket. 
    Build: Code Build, Jenkins
    Test: 3rd party tools, 
    Deploy: Code deploy, Cloud formation, Elastic bean stalk, ECS, S3 etc.

    It is made up of stages and each stages have sequential or parallel actions. 
    Can have manual approval process. 
    For each stage, generates output artifacts which will pass as input artifacts to next stage. 
    ATlest 2 stage is mandatory. 

Code Pipeline - Code Commit & Code Deploy: 
    In source, can select code commit, give the repository and branch details. 
    Note: Pipeline can created for 1 branch only. 
    Change detection Options: 
        Amazon cloudwatch events (recommended and automatically detect changes)
        AWS Code pipeline  (It periodically checks for changes)

    In code pipeline, can deploy to another region. 

Code Pipeline - Adding Code Build: 
    Can edit code pipeline. 
    Stage can be like source, build, test etc 
    Action group can be parallel or sequential. 

Code Pipeline - Artifacts, Encryption & S3: 
    Code pipeline -> Advanced setting: 
    S3: 
        Default location or Custom Location (recommended)    
    Key: 
        Default AWS Managed key or Customer managed key 

Code Pipeline - Manual approval steps:
    Under pipeline -> edit stage -> Manual approval stage. 
    Can select SNS topic to send notification. 
    
Code Pipeline - Cloud watch events integration: 
    During pipeline creation, it automatically create rules in event bridge. 
    Also can create our own rule, to choose target. 

Code Pipeline - Stage actions , Sequential & Parallel: 
    It consists of stages like source, build etc. 
    Sequential actions -> It process sequentially 
    Parallel actions -> It process parallel. 
    runOrder 1 
    runOrder 2 (It can be multiple and run parallel)
    runOrder 3 (It run after 2 and multiple can run parallel)

Code Pipeline - All Integrations: 
    https://docs.aws.amazon.com/codepipeline/latest/userguide/best-practices.html

    Build - Code build -> Generate artifacts 
    Test - Code build -> Does NOT generate artifacts. 

    Deploy -> Cloud formation (Comman use cases) 


Code Pipeline - Custom Action jobs with Lambda: 
    https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html

    1. Created the lambda service role. 
    2. Created the lambda function. 
    3. In code pipeline, INvoke a lambda function. 

Code Pipeline - Cloud Formation: 
    https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-cloudformation.html


-------------------------------------------------------------------------------------------------------------------------------

Code Star - Overview: 
    Just upload a project and Code star will create the pipeline automatically which includes code commit, code build, code deploy etc depends upon the project. 


-------------------------------------------------------------------------------------------------------------------------------

Jenkins Architecture: 
    Jenkins is open source CI-CD tool.
    Can replace code build, deploy, code pipeline. 
    Must deploy in master slave configuration. 
    Have jenkinsfile similar to buildspec.yml file. 
    Jenkins can replace code pipeline or other like code build, code deploy etc. 

Jenkins - Setup on EC2: 
    #!/bin/bash
    # setup Jenkins on EC2
    sudo yum update -y
    sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo
    sudo rpm --import https://pkg.jenkins.io/redhat/jenkins.io.key
    sudo yum install java-1.8.0 -y
    sudo amazon-linux-extras install epel
    sudo yum install jenkins -y
    sudo service jenkins start

    sudo cat /var/lib/jenkins/secrets/initialAdminPassword

    ec2-public-ip-address:8080 port 

    
Jenkis - AWS Plugins: 
    In Jenkins, have different plugins to integrate with AWS like AWS EC2, Code Pipeline etc. 

White Papers to Read: 
    Whitepapers to Read
        At this stage, please have a look at the following whitepapers to better help you in preparing for your certification. You don't need to read everything, just understand the general idea and skim through.

        MUST READ - Blue/Green Deployments on AWS
        https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf

        RECOMMENDED - Practicing Continuous Integration Continuous Delivery on AWS
        https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf

        RECOMMENDED - Jenkins on AWS
        https://d1.awsstatic.com/whitepapers/DevOps/Jenkins_on_AWS.pdf

        OPTIONAL - Introduction to DevOps on AWS
        https://d1.awsstatic.com/whitepapers/AWS_DevOps.pdf

        OPTIONAL - Development and Test on AWS
        https://d1.awsstatic.com/whitepapers/aws-development-test-environments.pdf


-------------------------------------------------------------------------------------------------------------------------------
                    CONFIGURATION MANAGEMENT & INFRASTRUCTURE AS CODE
-------------------------------------------------------------------------------------------------------------------------------

Cloud Formation: 
    Infrastructure as code 
    version control 
    Cant modify the existing stack. Can reupload again. 

Cloud Formation - Status: 
    CREATE_IN_PROGRESS 
    CREATE_COMPLETE 
    CREATE_FAILED
    UPDATE_IN_PROGRESS
    UPDATE_COMPLETE
    UPDATE_COMPLETE_CLEANUP_IN_PROGRESS 
    UPDATE_ROLLBACK_COMPLETE
    UPDATE_ROLLBACK_FAILED  -> it is important 
    DELETE_IN_PROGRESS
    DELETE_COMPLETE
    DELETE_FAILED
    ROLLBACK_IN_PROGRESS 
    ROLLBACK_COMPLETE 
    ROLLBACK_FAILED 


Cloud Formation - Create stacks hands on: 

    Cloudformation -> create stack -> S3 URL or Upload template. 
    Tags - Apply to the underlying resources. 
    Permissions - policy 
    Stack Failure - Roll back or Preserve. 
    Stack Policy - Defines the resources that want to protect from unintentional updates. 
    Timeout 
    Notifications 
    Termination protection 

    Estimate cost option. 


Cloud Formation - Update and Delete Stack: 
    Modify allow to upload new template or replace existing template. 
    Delting the stack, delete the underlying resources. 

Cloud Formation Parameters: 
    Parameters help to input custom values to stack. Some inputs cannot be determine at ahead of time. 

    Pseudo parameter are used to get the default values:  It refer by using !Ref AWS:AccountId 
        AWS::AccountId 
        AWS::NotificationARNs 
        AWS::NoValue 
        AWS::Region 
        AWS::StackId 
        AWS::StackName 

    Parameters:
        InstanceTypeParameter:
            Type: String
            Default: t2.micro
            AllowedValues:
            - t2.micro
            - m1.small
            - m1.large
            Description: Enter t2.micro, m1.small, or m1.large. Default is t2.micro.
            
Cloud Formation Resources: 

    It helps to create the resources. 
    AWS::aws-product-name::data-type-name 

    Resources: 
        myEC2:
            Type: AWS::EC2::Instance
            Properties: 
            ImageId: 'ami-0ed9277fb7eb570c9'
            InstanceType: !Ref InstanceTypeParameter

    Can create dynamic resource: 
        No, cannot generate. Everything has to be declared. 
    
    Is every AWS service supported: 
        Almost. For unavailable can use Lambda custom resources. 

Cloud Formation Mappings: 
    Used for known value in advance. 
    It is key value pair based. 

    !FindInMap[map_name, top_level_key, 2nd_level_key]      -> To retrieve the values from map. 

    Mappings: 
        Mapping01: 
            Key01: 
              Name: Value01
            Key02: 
              Name: Value02
            Key03: 
              Name: Value03
            
    RegionMap: 
        us-east-1:
            HVM64: ami-0ff8a91507f77f867
            HVMG2: ami-0a584ac55a7631c0c
        us-west-1:
            HVM64: ami-0bdb828fd58c52235
            HVMG2: ami-066ee5fd4a9ef77f1
        eu-west-1:
            HVM64: ami-047bb4163c506cd98
            HVMG2: ami-0a7c483d527806435
        ap-northeast-1:
            HVM64: ami-06cd52961ce9f0d85
            HVMG2: ami-053cdd503598e4a9d
        ap-southeast-1:
            HVM64: ami-08569b978cc4dfa10
            HVMG2: ami-0be9df32ae9f92309
                

Cloud Formation - Outputs: 
    If output is export (export name), then can import in another stack. (!importvalue export-name) 
    It enables cross stack. 
    Cant delete a stack if output is references in another stack. 

    Outputs: 
        Outputdisplayavailabilityzone: 
            Description: Output value description
            Value:  !GetAtt myEC2.AvailabilityZone
            Export: 
            Name: Availability-zone
        Outputdisplayinstanceid: 
            Description: Output value description
            Value:  !Ref myEC2
            Export: 
            Name: Instance-ID 

Cloud Formation - Conditions: 
    Can define the condition, if condition is true then only resource or output can be created. 
    Condition can reference another condition, parameter value or mapping. 

Cloud Formation Intrinsic Functions: 
        Fn::And
        Fn::Equals
        Fn::If
        Fn::Not
        Fn::Or
    Fn::Base64 -> returns base64 representation of input string. used to pass user data. 
    Fn::Ref or  !Ref   -> In resources, reference the resource ID. In parameters, references the value. 
    Fn:GetAtt or !GetAtt -> To get the attributes of the resources. 
    Fn::FindInMap or !FindInMap -> !FindInMap [map-name, top-key, 2nd-key]
    Fn::Join -> !Join [delimeter, [list of values ]]
    Fn::Sub ->  substitute 

Cloud Formation - User data: 
    It must be under Fn::Base64 
    User data is stored under /var/log/cloud-init log or /var/log/cloud-init-output.log 

          UserData:
            Fn::Base64: !Sub |
                #!/bin/bash -xe
                yum update -y 
                yum install cloud-init -y
                yum install httpd -y

Cloud Formation - cfn-init: 
    AWS::CloudFormation::Init must be in METADATA of a RESOURCE.
    It helps to make complex configuration readable. 
    EC2 instance query the cloudformation template to get the init data. 
    Logs captured in /var/log/cfn-init logs. 

Cloud Formation - cfn signal and wait conditions: 
    After cfn-init, still dont know whether ec2 metadata are properly installed or not. 
    for this, cfn-signal is used to tell cloudformation stack about the success. 
    Need to define WaitCondition, it blocks template from process and wait for signal. 
    cfn-signal to process followed by cfn-init. 

Cloud Formation - wait signal trouble shoot: 
    cfn init helper script not installed. 
    check the /var/log/cloud-init.log or cfn-init.log 
    disable rollback on failure to check the logs. 
    verify has internet connection. 

Cloud Formation - rollbacks: 
    During stack creation -> Stack failure options: 
        1. Rollback all stack resources. 
        2. Preserve successfully provisioned resources. 

Cloud Formation - Nested Stacks: 
    Nested stacks are stacks are created separately and referenced in stack. 
    As code grows and separate stacks for network, server, database etc. reference the stack with in another stack. 

    AWS::CloudFormation::Stack 
    CAPABILITY_AUTO_EXPAND 
    IAM_CAPABILITY 

Cloud formation - Change Sets: 
    It helps to verify the change before implement the stack. 
    Cloudformation -> During create process -> Create change set .
    After, Under change sets -> Execute to run the stack. 

Cloud Formation - Deletion Policy: 
    Retain 
    Snapshot 
    Delete 
    DeletionPolicy: Retain

Cloud Formation - Termination Protection: 
    Cloud Formation -> Creation -> Stack Creation Options -> Termination Protection (Enabled or Disabled)


Cloud Formation - Parameters from SSM: 

    Type:   AWS::SSM::Parameter                   -> To create the parameter. 
    Type:   AWS::SSM::Parameter::Value<String>    -> To get the value         
            AWS::SSM::Parameter::Value<List<String>>
            AWS::SSM::Parameter::Value<Any AWS type>
            'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'


Cloud Formation - Public Parameters from SSM: 
    aws ssm get-parameters-by-path --path /aws/service/ami-amazon-linux-latest --query 'Parameters[].Name' 
    This command helps to get the list of parameters.  Use this in the Default value. 

Cloud Formation - DependsOn: 
    Resources are created parallely. 
    DependsOn helps to create dependency between resources. 

Cloud Formation - Deploying Lambda Function: 
    Role ARN is required. 
    handler - index.handler_name 
    In lambda, 4000 characters have inline. Only node.js and python allowed as inline. 
    CAPABILITY_IAM is required when create the IAM role. 

    To get the source from S3 bucket: 
        place the code in index.py file and zipped to s3 bucket. 
              Code: 
                S3Bucket: devops-code-build-image-code-source
                S3Key: lambda-code.zip
                S3ObjectVersion: pXPqiF6GWRdBg3TKjZlHoNhCTKcB7qHh

Cloud Formation - Custom Resources: 
    Custom resources used in below scenarios: 
        1. Provisioning AWS resources that are not supported in cloud formation. 
        2. Provisioning non AWS Resources. 
        3. Perform provisioning steps not related to infrasture. 
                ex: initialize the database. 
    
    It consists of: 
        1. Create the logic for custom resources. 
        2. Deploy the custom resource to Lambda function. 
        3. Use the custom resource in cloud formation template that references lambda function or SNS topic. 
        
        To use a custom resource in a CloudFormation stack, you need to create a resource of either type AWS::CloudFormation::CustomResource or Custom::<YourName>. 
        ServiceToken: ARN of Lambda function. 

        Cloudformation runs logic at anytime during create, update and delete stacks. 


Cloud Formation - Drift Detection: 
    Cloudformation -> Stack actions -> Detect drift 
    Cloudformation -> Stack actions -> View drift results 

    If any change in cloudformation template and actual infrastructure. then capture in drift detection. 

Cloud Formation - Insufficient Capabilities Exception: 
    CAPABILITY_IAM or CAPABILITY_NAMED_IAM (for custom resources) -> To create IAM resources.  (InsufficientCApabilitiesException)
    CAPABILITY_AUTO_EXPAND -> For nested stack. 

Cloud Formation - cfn hup and metadata: 
    If any change in metadata, cfn-hub will detect the metadata changes with default interval of 15 mins and run the cloudformation::init metadata again. 
    /etc/cfn/cfn-hup.conf 
    /etc/cfn/hooks.d/cfn-auto-reloader.conf 

Cloud Formation - Stack Policies: 
    It provides fail safe mechanism to prevent accidental deletion. 
    It helps to prevent update or delee through stack policy. 
    It allow to update but during process failed with error "Action denied by stack policy: Statement [#1] does not allow [Update:*] for resource [*];"


-------------------------------------------------------------------------------------------------------------------------------

Elastic Bean Stalk: 
    It is easy to use for deploying and scaling web applications. 
    Simply upload the code and EBS automatically handles the deployment, load balancing, auto scaling to health monitoring. 
    No charge for EBS but only for underlying resources. 

Console Process:     
    Applications: 
        1. Create Application: 
            Name, Platform, Application (sample or upload own code)
    Environments: 
        Web server environment 
        Worker environment 
      
    Change History: 

Elastic Beanstalk - CLI: 
    https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-install-advanced.html   -> To run manually eb 

    https://github.com/aws/aws-elastic-beanstalk-cli-setup -> Execute scripts to install eb 

    Initialize EB CLI Project: 
        eb init         -> Initialize new project (give application name, platform)
        eb create       -> To create the environment. 
        eb status       -> To see the current status of environment. 
        eb health       -> To check the health information of instance in environment. 
        eb events       -> List of events output in EB. 
        eb logs         -> To pull the logs from instance in environment. 
        eb open         -> To open environment website in browser. 
        eb deploy       -> To deploy the application in environment. 
        eb config       -> to check the configuration details. 
        eb terminate    -> terminate the environment 
    
Elastic Beanstalk - Saved Configurations: 
    Save configuration helps to save the environment configuration details and later use the configuration to CREATE the new environment or LOAD to existing environment. 

    https://aws.amazon.com/blogs/devops/using-the-elastic-beanstalk-eb-cli-to-create-manage-and-share-environment-configuration/

    Environment -> Actions -> Save Configuration. 

    eb config save environment-name --cfg config-name   -> Save the current config of environment. 
    eb setenv ENABLE_COOL_NEW_FEATURE=true              -> Set the new config in current environment. 
    eb config --cfg config-name                         -> Update the config to the envionment. 

Elastic Beanstalk - .ebextensions for configs: 
    Under .ebextensions -> name.config file. 
    option_settings key used to modify EB configuration and define variables that can be retrieved from application using environemental variables. 

    option_settings:
        - namespace:  namespace
            option_name:  option name
            value:  option value
        - namespace:  namespace
            option_name:  option name
            value:  option value
            
    namespace is optional. 

    Precedence: 
        https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/command-options.html

        Default Value -> Configuration file (.ebextensions/name.confg) -> saved configuration -> Setting applied during creation/update thorugh console, CLI, SDK. 

Elastic Beanstalk - .ebextensions for Resources: 
    Under .ebextensions -> name.config file. 
    Can define the cloudformation template to create resources as part of elastic beanstalk. 

Elastic Beanstalk - RDS in or out of environment: 
    If RDS is part of EB, then will be deleted. 
    To Overcome: 
        1. Create external table. 
        2. Under EB Configuration -> Use key and value with dynamodb table name. 

Elastic Beanstalk - .ebextions for commands and containers: 
    commands: 
        run on instance before application setup. 
    container_commands: 
        It helps to modify source code. 
        It runs after application code is extracted and before the application  version deploy. 
        It run after application is setup. 
        database_migration: 
            leader_only: true -> to run only on single instance with leader only. 
            
Elastic Beanstalk - Good Features to know: 
    CLone Environment - To duplicate the environment. 
    Rebuild Environment - TO delete and recreate the environment. 
    Application -> Settings -> Application version lifecycle settings -> TO limit the no of versions for future deployments. 

Elastic Beanstalk- Rolling update strategy: 
    Environment -> COnfiguration -> Rolling updates and deployment. 
        Deployment Policy: 
            All at once -> Deploy all in one go. Fastest deployment. Downtime. No additional cost. 
            Rolling  -> update few instance at a time. Run below capacity. 
            Rolling with additional batch  -> Few additional instances. Run at capacity, small additional cost. Longer deploy. 
            Immutable   -> Temporary ASG is created. Higher cost. Longest deploy. Quick rollback. 
            Traffic Splitting 

            SWAP URL is used to shift traffic between environment. 

    https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html?icmpid=docs_elasticbeanstalk_console

    Deployment Modes: 
        Single Instance   -> Have single instance and good for dev. 
        High Availability -> Have load balancer and good for prod. 
    
Elastic Beanstalk - Rolling updates hands on: 
    Environment -> COnfiguration -> Rolling updates and deployment. :
        Application Deployments & Configuration updates. 

Elastic Beanstalk - SWAP URL: 
    Requires 2 environment and swap the URL between the environment. 
    1. Used clone option to created the similar environment. 
    2. Updated the source code. 
    3. eb deploy environment-name (to deploy the application to new environment)

    Environment -> Actions -> SWAP URL -> Select environment to swap. 

Elastic Beanstalk - Worker Environment: 
    For long running workloads or schedule using cron.yml 
    It polls from SQS. 

Elastic Beanstalk - Multi Docker Integration: 
    To create environment for docker: 
        Docker  -> Single container 
        Multi Container Docker: 
            Dockerrun.aws.json file -> Configuration file . 


-------------------------------------------------------------------------------------------------------------------------------


Lambda: 
    This is serverless. 
    Default create outside of VPC but can tag to VPC. 
    It has monitoring. 

Lambda - Sources and Use cases: 
    Source trigger can be API gateway, Cloud watch events, Alexa, Load balancer , Code commit, dynamodb, s3, kinesis etc. 

Lambda - Security, Environmental variables, KMS and SSM: 
    It support secrets managers, parameter store, environmental variables 

Lambda - Versions, Aliases and Canary Routing: 
    When work on lambda, $LATEST -> Mutable. 
    Version (code + configuration)  - Immutable. 
    $LATEST (mutable) -> Version V1 (immutable)
    Alias are pointers which point to versions and mutable. 
    Alias enabled blue green deployment by shifting the traffic to versions. 

Lambda - SAM Framework: 
    Serverless Application Model (SAM) is CLI tool helps to create and manage serverless service. 
    sam init        -> Initialize new project. 
    sam build       -> Build the project 
    sam package     -> Package the project. 
    sam deploy      -> Deploy the application. 

    SAM comes with build in Code Deploy to deploy the appliations. 

-------------------------------------------------------------------------------------------------------------------------------

Step Functions: 
    It helps to create workflow. 
    State machine defined using amazon state language. 
    It runs upto one year. 


-------------------------------------------------------------------------------------------------------------------------------

API Gateway: 
    It supports HTTP, REst API, WebSockets. 
    Act as front door. 
    It integrate with Lambda, Mock, Other AWS Services. 

    Endpoint Type : Regional, Edge Optimized Endpoint (edge location), Private (with vpc)

    Actions -> Create MEthods (GET, PUT etc )

    Integration Type: 
        Lambda Function 
        HTTP 
        Mock 
        AWS Service 
        VPC Link 

        Method Request/Method Response , Integration Request/Integration Response

        Deploy API -> Deploy to Stages 
    
    API Keys -> Create API Keys. 

    Authenticator - Using Cognito user Pools or Lambda authorizer. 

API Gateway - Integration with Lambda: 
    If proxy is enabled: 
        Lambda_Proxy -> Entire request passed to Lambda. 
            Modification is not allowed in Integration request. 
        
    Function_name = function_name:alias/version   (Can give alias or version in function name)
    Mapping template - It helps to modify the content of the request sent to function or return from function. 

API Gateway - Stages & Deployment: 
    Requires deployment for changes to effect. 
    Can create multiple stages. Each having their own stage variable (environment variable). 

API Gateway - Canary Deployments: 
    Can enable canary deployment for any stage. 
    Choose % of traffic that canary receives. 
    Promote Canary is used to deploy canary. 

    API Gateway -> Stages -> Canary -> Create canary. 
    Once canary created, then showing the stages with Canary options. 

API Gateway - Throttles: 
    10,000 request per second. 
    Usage Plan (Gives the request) -> Associate key with usage. 

API Gateway - Fronting Step Functions: 
    API Gateway -> AWS Services (Step Functions)


-------------------------------------------------------------------------------------------------------------------------------

ECS (Elastic Container Service):

Docker: 
    It is used to deploy apps. 
    Package apps in containers which run on any OS under the hood used Linux. 
    Docker images are stored in Docker Repositories like Docker hub (publlic), ECR (private).
    Docker used docker daemon engine to run the images (container) on same server. 
    Dockerfile used to create docker image. 

    docker build    -> Build image fromDockerfile 
    docker run      -> Run the container 
    docker push     -> Push to docker hub 
    docker pull     -> Pull from docker image

    It consists of: 
        ECS 
        Fargate 
        EKS 

ECS Clusters: 
    It is logical grouping of EC2 instances. 
    EC2 instances run the ECS agent (Docker Container) 
    ECS agent register the instance to clusters. 

    ECS -> Clusters -> Create Cluster: 
        Cluster Template: 
            Networking only (Fargate) 
            EC2 Linux + Networking 
            EC2 Windows + Networking 

    Under etc/ecs/ecs.config file: ECS_CLUSTER is used to register the instance to cluster. 
    cat /etc/ecs/ecs.config 
    ECS_CLUSTER=ecs-cluster-demo
    ECS_BACKEND_HOST=
    ECS Instances under the cluster. 

    It create the instance as Auto scaling group. If require to modify the instances, can change in auto scaling group. 


ECS Task Definition: 
    It contains metadata in json format which tells ECS how to run container. 
    It contains crucial information around: 
        Image name 
        Port binding for container and hosts. 
        Memory and CPU required. 
        Environment variables. 
        Networking Information. 

        Task Role is required to perform the tasks. 

        Container Definitions: 
            Image, port 

        ECS -> Task Definition -> EC2 -> 
            Task Role - Optional IAM role that tasks can use to make API requests to authorized AWS services. Create an Amazon Elastic Container Service Task Role.
            Task execution role - This role is required by tasks to pull container images and publish container logs to Amazon CloudWatch on your behalf.
        

ECS Service: 
    It define how many tasks to run and how to run. 

    Deployment Type: ROlling update & Blue greem powered by code deploy. 
    Task Placement: 
        AZ Balanced Spread 
        AZ Balanced Bin pack 
        BinPack 
        One Task Per host 
        Custom 

    ECS -> Cluster -> Services -> Create -> 


ECS Service with Load Balancers: 
    Application Load Balancer - Dynamic port forwarding - Allows to route to dynamic port. 
    Under Services -> ALB 


ECR Part 1:
    ECR is private docker image repository. 
    $(aws ecr get-login --no-include-email region us-east-1) 
    docker push 
    docker pull 

ECR Part 2: 
    Here updated the container image with private repository image. 


FARGATE: 
    In EC2 , requires to create cluster. 
    In fargate, no need of cluster which is serverless. 


ECS & Multi Docker beanstalk: 
    Elastic Beanstalk: 
        Single container    -> Run EC2 
        Multi Container     -> Run ECS 
    
    Requires Dockerrun.aws.json file under root of source code. 

ECS- IAM Roles: 
    ecsInstanceRole -> EC2 Instances for ECS Instances. 
    TaskRole -> Optional IAM role that tasks can use to make API requests to authorized AWS services. Create an Amazon Elastic Container Service Task Role.
    Task execution role - This role is required by tasks to pull container images and publish container logs to Amazon CloudWatch on your behalf.

ECS - AutoScaling: 
    Service -> COnfigure Auto Scaling. 
    Requires 2 auto scaling for ECS instances and Container instances. 

ECS - Cloud Watch Integrations: 
    Container Definition -> Integrate to cloud. -> Helps container send the log to cloud watch directly. 
    For ECS instances, install cloud watch log agent in ecs instances. 

ECS - Code Pipeline - CICD: 
    ecsworkshop.com 


------------------------------------------------------------------------------------------------------------------------------

OpsWorks: 
    It is configuration management with Chef and Puppet. 
    It helps to build highly dynamic application and propagate changes instantly. 
    It consists of: 
        OpsWorks Stacks -> Define, group, provision and operate application in AWS by using chef in local mode.
        OpsWorks for Chef Automate -> Create chef servers and use chef tooling to automate them. 
        OpsWork for Puppet Enterprise -> Create puppet server which monitor the infrastructure. 
    
    Stack is a set of layers, instances and related AWS resources whose configuration want to manage together. 

    Ops Work Stacks -> Stacks ->  Create Stack. 

    A layer is a blueprint for a set of Amazon EC2 instances. It specifies the instance's settings, associated resources, installed packages, profiles, and security groups. You can also add recipes to lifecycle events of your instances, for example: to set up, deploy, configure your instances, or discover your resources.

    Opsworks stack: 
        Elastic Loadbalancing Layer 
        Application server layer  -> Cookbook repository is used to provision. 
        Amazon RDS layer 
    
    Layers: 
        Auto healing enabled : yes 

    Instances: 
        Time based -> can select the time/schedule when to run. 
        Load based -> runs based on the load. 
        24/7  -> runs 24/7 

Life cycle events: 
    Setup       -> After instance startup and booting. 
    Configure   -> It occurs on ALL of instances. An instance enters or leaves online, Associate or Disassocite elastic IP address, Attach or Detach ELB from instance. 
    Deploy      -> It occurs when run deploy command. to deploy the application. 
    Undeploy    -> To remove the app from instances. 
    Shutdown    -> Shutdown happens before terminating. 

Auto healing & Cloud watch events: 
    If any failure, it will recreate the instances. 


------------------------------------------------------------------------------------------------------------------------------
                                MONITORING AND LOGGING
------------------------------------------------------------------------------------------------------------------------------

Cloud Trail: 
    It continuosly logs AWS accounts activity. 
    It tracks API call details. 
    It send to S3 bucket, SNS Notification or Send to Cloudwatch logs. 

    Cloud Trail -> Name, S3 bucket, 
        Log file validation - To verify whether log file modified/deleted. 
        SNS Notification delivery -> To notify in SNS for each log delivered. 
        Event Type: 
            Management Events -> Capture management operations performed on AWS resources. 
            Data Events -> Log the resource operations performed on/with in resources. 
            Insight Events -> Identity unusual activity, errors or user behaviour. 

Cloud Trail - Log Integrity: 
    Log file validation - To verify whether log file modified/deleted. 
    aws cloud-trail validate logs -> Command 
    Digest file deliver every 1 hour and then run the validate log command. 

Cloud Trail - Cross Account Logging: 
    To allow multiple account to send cloud trail logs to S3 bucket. 
        1. Requires to update the S# bucket policy to allow other account. 
        2. In other accounts, choose existing bucket name which created in another account. 

------------------------------------------------------------------------------------------------------------------------------

Kinesis - Data Streams Overview: 
    It is alternative to apache kafka. 
    It is used for real time stream. 
    It consists of: 
        Kinesis stream      -> Low latency streaming ingest atscale. 
        Kinesis Analytics   -> Perform real time analytics on stream. 
        Kinesis Firehose    -> Load streams into S3, Redshift, Elasticsearch 

        Producers -> SHARDS -> Consumers
    Multiple application can consume the same stream. 
    Data retention is 1 day upto 7 days. 
    Records are ordered per shard. 
    Producer: 
        It consists of Record Key and Data blob and sequence nbr. 
        1 mb/s or 1000 messages per second write. 
    Consumer: 
        2 mb/s read. 
        5 api calls per second per shard. 
    
    Kinesis SDK, Agent, Cloud Watch Logs -> Kinesis Stream 
    AWS Kinesis KCL: (Kinesis Client Library): 
        It uses dynamodb to checkpoint offsets. 
        It uses dynamodb to track other workers and share work amoing shards. 
        great for reading in distributed manner. 
    
Kinesis Alalytics & Firehose: 
    Firehose: 
        It is fully managed and in near real time (60 sec latency)
        It loads data to S3, Redshift, Splunk, ELasticsearch. 
        Data transformation through AWS Lambda. 
        Supports compression when target is S3. 
    Analytics: 
        It perform real time analytics using SQL. 

    Kinesis -> Kinesis Data Firehose -> Create delivery stream -> Select source, destination


------------------------------------------------------------------------------------------------------------------------------

Cloud Watch Metrics:

    It is to track metrics like cpu for all aws services. 
    Detailed monitoring - Metrics comes in every 1 minute and to pay. 
    Basic monitoring - Every 5 minutes and no charges. 
    Upto 15 months maximum to check the metrics. 

        < 60 seconds data available for 3 hours. 
        1 minute data available for 15 days. 
        5 minutes data available for 63 days 
        1 hour data available for 15 months. 

    Cloud watch -> Metrics 

    Cloud watch agent used to get EC2- RAM details. 
    In EBS, How much space left can get usin cloud watcg agent. 
    In ASG, Group Metrics not enabled by default. 

Cloud Watch Custom Metrics: 
    Standard Resolution     -> Granularity of 1 minute. 
        High Resolution         -> Granularity of 1 sec. 
        Can publish custom petrics using API or CLI using putmetricdata call. 

        aws cloudwatch put-metric-data --metric-name Buffers --namespace MyNameSpace --unit Bytes --value 231434333 --dimensions InstanceId=1-23456789,InstanceType=m1.small

        aws cloudwatch get-metric-statistics --metric-name Buffers --namespace MyNameSpace --dimensions Name=InstanceId,Value=1-23456789 Name=InstanceType,Value=m1.small --start-time 2021-12-23T04:00:00Z --end-time 2021-12-23T07:00:00Z --statistics Average --period 60

Cloud Watch Metrics - Exports: 

    aws cloudwatch get-metric-statistics --metric-name Buffers --namespace MyNameSpace --dimensions Name=InstanceId,Value=1-23456789 Name=InstanceType,Value=m1.small --start-time 2021-12-23T04:00:00Z --end-time 2021-12-23T07:00:00Z --statistics Average --period 60

    Use cloud watch events to trigger lambda for every specific schedule. Lambda is used to export metrics to S3. 

------------------------------------------------------------------------------------------------------------------------------

Cloud Watch Alarms: 

    Based on metrics, it trigger the alarm. 
    Cannot combine multiple metrics for one alarm. One metric for one alarm. 

    ALARM           -> Outside of threshold. 
    INSUFFICIENT    -> Insufficient data. 
    OK              -> Within Threshold. 

    Cloud Watch -> Alarm -> Create Alarm 

    Send notification to SNS. 

    In Alarm: 
        Auto scaling actions
        EC2 Action 
        Systems Manager Action -> Create Incidient or OpsItem 

Cloud Watch Alarm - Billing Alarm: 

    Cloud watch -> Alarms -> Billing (Metric Name - EstimatedCharges)
    It can be at global level or service level. 

------------------------------------------------------------------------------------------------------------------------------

Cloud watch Logs - Overview: 

    Cloud Watch -> Logs -> Log Group -> Log Streams -> Logs 
    /aws/service-name/  -> Managed by AWS 
    Expire Events After Option -> To set expiry date. 
    Can create or delete log stream. 

Cloud Watch Unified Agent: (Send metrics and logs) 

    https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/UseCloudWatchUnifiedAgent.html
    https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html

    It uses the ssm parameter store to keep the config details. 

Cloud Watch Logs - Metric Filters & ALarms: 
    Cloud watch -> Log group -> Create metric filter : 
        Define pattern - like Error , Info etc 
        Assign a metric 

Cloud Watch Logs - Export to S3: 
    Cloud watch -> Logs -> Actions -> Export data to Amazon S3. 
    It is done manually. 

Cloud Watch Logs - Subscriptions: 
    Cloud watch -> Logs -> Actions -> Subscriptions -> : 
        Kinesis, Kinesis Firehose, Lambda, Amazon opensearch service. 

------------------------------------------------------------------------------------------------------------------------------

CLoud Watch Events: 

    
















    














